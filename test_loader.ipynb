{"cells":[{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UwngXXzOZMNy","executionInfo":{"status":"ok","timestamp":1679495233358,"user_tz":-540,"elapsed":5549,"user":{"displayName":"박상우","userId":"00596815453416531190"}},"outputId":"a0e2405e-2b24-4519-b563-eaf84e3fe4fd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: py-hanspell in /usr/local/lib/python3.9/dist-packages (1.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from py-hanspell) (2.27.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->py-hanspell) (3.4)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->py-hanspell) (2.0.12)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->py-hanspell) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->py-hanspell) (1.26.15)\n"]}],"source":["# Uncomment below section and run in case of re-connecting Colab\n","\n","# !pip install git+https://github.com/haven-jeon/PyKoSpacing.git\n","# !pip install transformers\n","# !pip install git+https://github.com/ssut/py-hanspell.git\n","# !pip install py-hanspell\n","\n","# from google.colab import drive\n","# drive.mount('/content/drive')\n","\n","# %cd drive/MyDrive/MBTI\n","# !pwd"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"AKqHJ0tPeJMc","executionInfo":{"status":"ok","timestamp":1679493580580,"user_tz":-540,"elapsed":13724,"user":{"displayName":"박상우","userId":"00596815453416531190"}}},"outputs":[],"source":["import os\n","import re\n","\n","import torch\n","import pandas as pd\n","from torch.utils.data import Dataset, DataLoader\n","from pykospacing import Spacing\n","from hanspell import spell_checker\n","from transformers import AutoTokenizer\n","\n","pd.set_option('display.width', 180)\n","\n","#TODO:\n","# * train / text 다른 로직이 필요 (데이터 형식이 조금 다름)\n","# * 다른 Text preprocess 방식 도입 검토\n","\n","class ddDataset(Dataset):\n","    def __init__(\n","        self,\n","        data_path: str,\n","        question_path: str,\n","        pretrained_url: str = \"klue/bert-base\",\n","        target_mbti: str = None,\n","        txt_preprocess: bool = True,\n","        normalize: bool = True,\n","        is_binary_classification: bool = True,\n","        is_bert: bool = True,\n","        is_train: bool = True\n","        ):\n","        \"\"\"DataLoader for MBTI dataset\n","\n","        Args:\n","            data_path (str): Data file path. Both csv and parguet files are allowed.\n","            question_path (str): Question file path. Both csv and parguet files are allowed.\n","            target_mbti (str): Target mbti for binary classification.\n","            txt_preprocess (bool, optional): Text preprocessing pipeline. (e.g. fixing grammar, removing punctuations). Defaults to True.\n","            normalize (bool, optional): Normalize numeric attribute. Defaults to True.\n","            is_binary_classification (bool, optional): Target of task. You can choose btw Multi-class classificaiton\n","                and 4 binary classification problem. Defaults to True.\n","            is_bert (bool, optional): Using BERT for language model or not. Defaults to True.\n","            is_train (bool, optional): Whether given data is for training or not. Defaults to True.\n","        \"\"\"\n","\n","        data = pd.read_csv(data_path) if data_path.endswith('.csv') else pd.read_parquet(data_path)\n","        self.question_data = pd.read_csv(question_path)\n","\n","        print(\"data :\", data.head())\n","        print(\"--------origin---------\")\n","        data_origin = data.copy()\n","\n","        # preprocess data\n","        if txt_preprocess:\n","            # data['Answer'] = data['Answer'].apply(self.fix_grammar)\n","            self.preprocess_text(data)\n","        if normalize:\n","            data['Age'] = (data['Age'] - data['Age'].mean()) / data['Age'].std()\n","\n","        print(\"--------preprocess---------\")\n","\n","        # align dataset with binary classification (only for training data - test data doesn't contain 'MBTI' field)\n","        label_col = None\n","        if is_train and is_binary_classification:\n","            label_col = self.prepare_binary_classification(data, target_mbti)\n","            l_list = label_col.split('/')\n","            assert data[label_col].value_counts()[0] == \\\n","                  data[label_col].value_counts()[1]\n","\n","        print(\"data :\", data.head())\n","        print(\"--------prepare_binary_classification---------\")\n","\n","        # prepare for language model\n","        #TODO: tokenizer class 인자로 넣어야 함 & tokenizer 만으로 [CLS], [SEP] 잘 붙는지 확인 필요\n","        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_url)\n","        data = self.tokenize(data)\n","\n","\n","        selected_question = self.question_data.iloc[data_origin['Q_number'][0] - 1].Question\n","        selected_answer = data_origin['Answer'][0]\n","        print(selected_question)\n","        print(selected_answer)\n","        encoding = self.tokenizer(selected_question, selected_answer)\n","        print('input_ids :', encoding['input_ids'])\n","        print('token_type_ids :', encoding['token_type_ids'])\n","        print('attention_mask :', encoding['attention_mask'])\n","        decoding = self.tokenizer.convert_ids_to_tokens(encoding['input_ids'])\n","        print(decoding)\n","\n","        print(\"data :\", data.head())\n","        print(\"--------tokenize---------\")\n","\n","        # select columns for both training and inference\n","        #TODO: 테스트 데이터를 고려해서 유저 정보를 학습에 활용하지 않는 상황. 필요시 고쳐야 함.\n","        self.cat_col    = ['Gender']\n","        self.num_col    = ['Age']\n","        self.label_col  = label_col\n","        self.data       = data\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        #TODO:\n","        # * getitem --> BERT input dimension 확인 후 변환 & 다른 features 들과 붙여서 input instance 생성\n","\n","        selected_data = self.data.iloc[idx]\n","\n","        input_ids      = torch.tensor(selected_data['QandA']['input_ids'])\n","        token_type_ids = torch.tensor(selected_data['QandA']['token_type_ids'])\n","        attention_mask = torch.tensor(selected_data['QandA']['attention_mask'])\n","        label          = torch.tensor(selected_data[self.label_col])                        # [batch size   x   1]\n","\n","        # txt_input = (input_ids, token_type_ids, attention_mask)                             # [batch size   x   sequence length]  <-- input_ids (actual input)\n","        cat_input = torch.tensor(selected_data[self.cat_col])                               # [batch size   x   # categorical features]\n","        num_input = torch.tensor(selected_data[self.num_col])                               # [batch size   x   # numerical features]\n","\n","        # sample = {\"txt_input\": txt_input, \"cat_input\" : cat_input, \"num_input\" :num_input, \"label\": label}\n","\n","        sample = selected_data['QandA']\n","        sample['cat_input'] = cat_input\n","        sample['num_input'] = num_input\n","        sample['label'] = label\n","\n","        print('original len :', len(sample['input_ids']))\n","\n","        return sample\n","\n","    # ======================\n","    #    Helper Functions\n","    # ======================\n","\n","    def fix_grammar(self, answer: str) -> str:\n","        answer = spell_checker.check(answer)\n","        return answer.checked\n","\n","    def fix_spacing(self, answer: str) -> str:\n","        answer = answer.replace(\" \", '')\n","        spacing = Spacing()\n","        return spacing(answer)\n","\n","    def remove_punctuation(self, answer: str) -> str:\n","        #FIXME: remove punctuation 검증 필요\n","        answer = re.sub(r'[@%\\\\*=()/~#&\\+á?\\xc3\\xa1\\-\\|\\.\\:\\;\\!\\-\\,\\_\\~\\$\\'\\\"]', '', answer)\n","        answer = re.sub(r\"^\\s+\", '', answer)                    # remove space from start\n","        answer = re.sub(r'\\s+$', '', answer)                    # remove space from the end\n","        return answer\n","\n","    def preprocess_text(self, data: pd.DataFrame):\n","        # data['Answer'] = data['Answer'].apply(self.fix_grammar)\n","        data['Answer'] = data['Answer'].apply(self.fix_spacing)\n","        data['Answer'] = data['Answer'].apply(self.remove_punctuation)\n","\n","    def prepare_binary_classification(self, data: pd.DataFrame, target_mbti: str) -> str:\n","        t_value, f_value = 1, 0\n","        target_mbti = target_mbti.upper()\n","        if target_mbti not in ['E', 'N', 'F', 'P']:\n","            if target_mbti in ['I', 'S', 'T', 'J']:\n","                t_value, f_value = 0, 1\n","            else:\n","                raise ValueError (\"Wrong mbti type. Try different type instead.\")\n","\n","        data['MBTI'] = data['MBTI'].str     \\\n","            .contains(target_mbti)          \\\n","                .replace({True: t_value, False: f_value})\n","\n","        col_name = None\n","        if target_mbti in ('E', 'I'):\n","            col_name = 'I/E'\n","        elif target_mbti in ('N', 'S'):\n","            col_name = 'S/N'\n","        elif target_mbti in ('F', 'T'):\n","            col_name = 'T/F'\n","        else:\n","            col_name = 'J/P'\n","        data.rename(columns = {'MBTI':col_name}, inplace=True)\n","        \n","        return col_name\n","\n","    def tokenize(self, data: pd.DataFrame):\n","\n","        def tokenize_per_sentence(series: pd.Series) -> str:\n","            selected_question = self.question_data.iloc[series['Q_number'] - 1].Question\n","            selected_answer = series['Answer']\n","            \n","            return self.tokenizer(selected_question, selected_answer)\n","        \n","        data['QandA'] =  data.apply(tokenize_per_sentence, axis=1)\n","        data = data.drop(labels='Answer', axis=1)\n","        return data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7y5-Zpjgq_KS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679420173418,"user_tz":-540,"elapsed":11215,"user":{"displayName":"박상우","userId":"00596815453416531190"}},"outputId":"c56de4de-63dd-427b-ba3f-88f73950cdad"},"outputs":[{"output_type":"stream","name":"stdout","text":["data :    Data_ID  User_ID  Gender  Age  MBTI  Q_number                                             Answer\n","0        1        1       1   30  INFP         1  <아니다> 어릴 때 왕따 당한 경험이 있고 외부 활동을 좋아하지 않기 때문에 소수의...\n","1        2        1       1   30  INFP         2  <중립>  다양한 관심사를 탐구하진 않지만 대체로 자연과 역사에 관련된 것을 좋아하...\n","2        3        1       1   30  INFP         3  <그렇다> 감정 이입이 잘되어 코미디 영화에서 사람이 울고 있을 때도 울기 때문에 ...\n","3        4        1       1   30  INFP         4  <중립> 대비책을 세우긴 하는데 세우다가 마는 편입니다. 일의 변수가 생길 수 있고...\n","4        5        1       1   30  INFP         5  <아니다> 평정심을 유지 못 하는 편입니다. 머릿속은 백지화가 된 상태로 말도 제대...\n","--------origin---------\n","--------preprocess---------\n","data :    Data_ID  User_ID  Gender       Age  I/E  Q_number                                             Answer\n","0        1        1       1 -0.994778    0         1  <아니다> 어릴 때 왕따 당한 경험이 있고 외부 활동을 좋아하지 않기 때문에 소수의...\n","1        2        1       1 -0.994778    0         2  <중립> 다양한 관심사를 탐구하진 않지만 대체로 자연과 역사에 관련된 것을 좋아하며...\n","2        3        1       1 -0.994778    0         3  <그렇다> 감정이입이 잘 되어 코미디 영화에서 사람이 울고 있을 때도 울기 때문에 ...\n","3        4        1       1 -0.994778    0         4  <중립> 대비책을 세우긴 하는데 세우다가 마는 편입니다일의 변수가 생길 수 있고 변...\n","4        5        1       1 -0.994778    0         5  <아니다> 평정심을 유지 못하는 편입니다 머릿속은 백지화가 된 상태로 말도 제대로 ...\n","--------prepare_binary_classification---------\n","주기적으로 새로운 친구를 만드나요? 경험을 비추어봤을 때 어떤지와 그러한 이유가 궁금해요.\n","<아니다> 어릴 때 왕따 당한 경험이 있고 외부 활동을 좋아하지 않기 때문에 소수의 친구와만 지냅니다.\n","input_ids : [2, 7267, 11187, 3824, 3949, 2138, 4577, 2075, 2182, 35, 4043, 2069, 9695, 2051, 3072, 2069, 904, 19093, 2522, 3626, 2470, 3782, 2116, 5333, 2097, 2182, 18, 3, 32, 3614, 2062, 34, 8115, 904, 18149, 6391, 4043, 2052, 1513, 2088, 4993, 3746, 2069, 6992, 2118, 1380, 2015, 3624, 2170, 6700, 2079, 3949, 2522, 2154, 1583, 26775, 18, 3]\n","token_type_ids : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","attention_mask : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","['[CLS]', '주기', '##적으로', '새로운', '친구', '##를', '만드', '##나', '##요', '?', '경험', '##을', '비추', '##어', '##봤', '##을', '때', '어떤지', '##와', '그러', '##한', '이유', '##가', '궁금', '##해', '##요', '.', '[SEP]', '<', '아니', '##다', '>', '어릴', '때', '왕따', '당한', '경험', '##이', '있', '##고', '외부', '활동', '##을', '좋아하', '##지', '않', '##기', '때문', '##에', '소수', '##의', '친구', '##와', '##만', '지', '##냅니다', '.', '[SEP]']\n","data :    Data_ID  User_ID  Gender       Age  I/E  Q_number                                        QandA\n","0        1        1       1 -0.994778    0         1  [input_ids, token_type_ids, attention_mask]\n","1        2        1       1 -0.994778    0         2  [input_ids, token_type_ids, attention_mask]\n","2        3        1       1 -0.994778    0         3  [input_ids, token_type_ids, attention_mask]\n","3        4        1       1 -0.994778    0         4  [input_ids, token_type_ids, attention_mask]\n","4        5        1       1 -0.994778    0         5  [input_ids, token_type_ids, attention_mask]\n","--------tokenize---------\n"]}],"source":["data_path = \"data/example_train.csv\"\n","question_path = \"data/question_filtered.csv\"\n","\n","data = ddDataset(\n","   data_path,\n","   question_path,\n","   target_mbti = \"E\"\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1679323158528,"user":{"displayName":"박상우","userId":"00596815453416531190"},"user_tz":-540},"id":"QgORnPjREzUA","outputId":"c0dd9fb9-0074-4d45-902a-265c9256aeb1"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'input_ids': [2, 7267, 11187, 3824, 3949, 2138, 4577, 2075, 2182, 35, 4043, 2069, 9695, 2051, 3072, 2069, 904, 19093, 2522, 3626, 2470, 3782, 2116, 5333, 2097, 2182, 18, 3, 32, 3614, 2062, 34, 8115, 904, 18149, 6391, 4043, 2052, 1513, 2088, 4993, 3746, 2069, 6992, 2118, 1380, 2015, 3624, 2170, 6700, 2079, 3949, 2522, 10873, 26775, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n","tensor([    2,  7267, 11187,  3824,  3949,  2138,  4577,  2075,  2182,    35,\n","         4043,  2069,  9695,  2051,  3072,  2069,   904, 19093,  2522,  3626,\n","         2470,  3782,  2116,  5333,  2097,  2182,    18,     3,    32,  3614,\n","         2062,    34,  8115,   904, 18149,  6391,  4043,  2052,  1513,  2088,\n","         4993,  3746,  2069,  6992,  2118,  1380,  2015,  3624,  2170,  6700,\n","         2079,  3949,  2522, 10873, 26775,     3])\n","Gender    1\n","Name: 0, dtype: object\n"]}],"source":["qanda = data.data.iloc[0]\n","print(qanda['QandA'])\n","print(torch.tensor(qanda['QandA']['input_ids']))\n","print(qanda[['Gender']])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1679320318512,"user":{"displayName":"박상우","userId":"00596815453416531190"},"user_tz":-540},"id":"t0PdBrXoLJpf","outputId":"8ba40460-dcd7-4471-8dda-85217e6a7628"},"outputs":[{"name":"stdout","output_type":"stream","text":["96\n","   Data_ID  User_ID  Gender       Age  I/E  Q_number                                        QandA\n","0        1        1       1 -0.994778    0         1  [input_ids, token_type_ids, attention_mask]\n","1        2        1       1 -0.994778    0         2  [input_ids, token_type_ids, attention_mask]\n","2        3        1       1 -0.994778    0         3  [input_ids, token_type_ids, attention_mask]\n","3        4        1       1 -0.994778    0         4  [input_ids, token_type_ids, attention_mask]\n","4        5        1       1 -0.994778    0         5  [input_ids, token_type_ids, attention_mask]\n"]}],"source":["print(len(data))\n","print(data.data.head())"]},{"cell_type":"code","source":["# 불러와지는지 확인\n","from dataloader import MBTIDataset\n","\n","env_dict = {\n","    # ==== Arguments of dataset =====\n","    'train_path'        : './data/example_train.csv',\n","    'question_path'     : './data/question_filtered.csv',\n","    'target'            : 'E',\n","    'pretrained_url'    : \"klue/bert-base\",\n","    'padding_per_batch' : True,\n","    # ==== Arguments of dataloader =====\n","    'batch_size'        : 64,\n","    'shuffle'           : True\n","}\n","\n","# Dataset\n","train_dataset = MBTIDataset(\n","    data_path           = env_dict['train_path'],\n","    question_path       = env_dict['question_path'],\n","    target_mbti         = env_dict['target'],\n","    pretrained_url      = env_dict['pretrained_url'],\n","    padding_per_batch   = env_dict['padding_per_batch'],\n","    is_train            = True\n",")\n","\n","print(len(train_dataset))\n","print(train_dataset.data.head())"],"metadata":{"id":"8a2XIoO9Mdcd"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3C7jFlk4P5pn"},"outputs":[],"source":["from torch.utils.data import DataLoader\n","from transformers import DataCollatorWithPadding\n","\n","# define collator function when padding per batch is needed\n","#TODO: data_collator 잘 작동하는지 확인 필요\n","data_collator = DataCollatorWithPadding(tokenizer=data.tokenizer)\n","\n","# Dataloader\n","train_dataloder = DataLoader(\n","    data,\n","    batch_size = 16,\n","    shuffle = False,\n","    collate_fn = data_collator\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1679324180612,"user":{"displayName":"박상우","userId":"00596815453416531190"},"user_tz":-540},"id":"Gu3KJ2ymQN3J","outputId":"cb494306-db51-467a-e78b-d23967ce8cbb"},"outputs":[{"name":"stdout","output_type":"stream","text":["original len : 56\n","original len : 83\n","original len : 92\n","original len : 92\n","original len : 79\n","original len : 87\n","original len : 100\n","original len : 48\n","original len : 104\n","original len : 77\n","original len : 70\n","original len : 76\n","original len : 63\n","original len : 84\n","original len : 77\n","original len : 70\n","{'input_ids': tensor([[    2,  7267, 11187,  ...,     0,     0,     0],\n","        [    2,  3936,  3641,  ...,     0,     0,     0],\n","        [    2,  3656,  3611,  ...,     0,     0,     0],\n","        ...,\n","        [    2,  4051,  4362,  ...,     0,     0,     0],\n","        [    2,  3656,  3611,  ...,     0,     0,     0],\n","        [    2,  3971,  3746,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        ...,\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]]), 'cat_input': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'num_input': tensor([-0.9948, -0.9948, -0.9948, -0.9948, -0.9948, -0.9948, -0.9948, -0.9948,\n","        -0.9948, -0.9948, -0.9948, -0.9948, -0.9948, -0.9948, -0.9948, -0.9948],\n","       dtype=torch.float64), 'labels': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}\n"]}],"source":["for step, batch in enumerate(train_dataloder):\n","  # input_ids, token_type_ids, attention_mask = batch\n","  # print(batch[\"token_type_ids\"])\n","  # print(torch.count_nonzero(batch[\"token_type_ids\"][0]))\n","  print(batch)\n","  # print('revised len :', len(batch['input_ids']))\n","  # print(batch[\"input_ids\"].shape)\n","  break"]},{"cell_type":"code","source":["import os\n","import re\n","from typing import Union\n","\n","import torch\n","import pandas as pd\n","from torch.utils.data import Dataset\n","from pykospacing import Spacing\n","from hanspell import spell_checker\n","from transformers import AutoTokenizer\n","\n","#TODO:\n","# 1. train / text 다른 로직이 필요 (데이터 형식이 조금 다름)\n","# 2. 다른 Text preprocess 방식 도입 검토\n","#       ref : https://ebbnflow.tistory.com/246\n","\n","class IIIDataset(Dataset):\n","    def __init__(\n","        self,\n","        data_path     : Union[str, pd.DataFrame],\n","        question_path : Union[str, pd.DataFrame],\n","        txt_preprocess: bool            = True,\n","        normalize     : bool            = True,\n","        pretrained_url: str             = \"klue/bert-base\",\n","        padding_per_batch               = True,\n","        is_binary_classification: bool  = True,\n","        is_bert       : bool            = True,\n","        is_train      : bool            = True\n","        ):\n","        \"\"\"DataLoader for MBTI dataset\n","\n","        Args:\n","            data_path (str): Data file path. Both csv and parguet files are allowed.\n","            question_path (str): Question file path. Both csv and parguet files are allowed.\n","            txt_preprocess (bool, optional): Text preprocessing pipeline. (e.g. fixing grammar, removing punctuations). Defaults to True.\n","            normalize (bool, optional): Normalize numeric attribute. Defaults to True.\n","            is_binary_classification (bool, optional): Target of task. You can choose btw Multi-class classificaiton\n","                and 4 binary classification problem. Defaults to True.\n","            is_bert (bool, optional): Using BERT for language model or not. Defaults to True.\n","            is_train (bool, optional): Whether given data is for training or not. Defaults to True.\n","        \"\"\"\n","\n","        def resolve_path(path:str)->pd.DataFrame:\n","            if path.endswith('.csv'):\n","                try:\n","                    df = pd.read_csv(path)\n","                except:\n","                    df = pd.read_csv(path, encoding='cp949')\n","            else:\n","                df = pd.read_parquet(path)\n","            return df\n","\n","        data = None\n","        question_data = None\n","        # if given data_path is pd.Dataframe, we assume preprocessing is already applied to given Dataframe\n","        # so that it can skip all the processes below\n","        if not isinstance(data_path, pd.DataFrame):\n","            data = resolve_path(data_path)\n","            question_data = resolve_path(question_path)\n","\n","            self.question_data = question_data\n","\n","            # preprocess data\n","            if txt_preprocess:\n","                self.preprocess_txt(data)\n","            if normalize:\n","                data['Age'] = (data['Age'] - data['Age'].mean()) / data['Age'].std()\n","\n","            #FIXME: 4개의 Dataset 생성은 번거로움. 4개의 Column 을 생성하도록 바꾸기\n","            # make dataset suitable for binary classification (only for training data - test data doesn't contain 'MBTI' field)\n","            label_cols = None\n","            if is_train and is_binary_classification:\n","                self.prepare_binary_classification(data)\n","                # if method right above works successfully, then data should contain same # 0 and 1.\n","                label_cols = ['I/E', 'S/N', 'T/F', 'J/P']\n","                for col in label_cols:\n","                    value_counted = data[col].value_counts()\n","                    assert value_counted[0] == value_counted[1]\n","\n","            # prepare for language model\n","            self.tokenizer = AutoTokenizer.from_pretrained(pretrained_url)\n","            self.padding_per_batch = padding_per_batch\n","            self.tokenize(data)\n","\n","        else:\n","            data = data_path\n","\n","        # set columns for both training and inference\n","        #TODO: 테스트 데이터를 고려해서 유저 정보를 학습에 활용하지 않는 상황. 필요시 고쳐야 함\n","        self.cat_col    = ['Gender']\n","        self.num_col    = ['Age']\n","        self.label_cols = ['I/E', 'S/N', 'T/F', 'J/P']\n","        self.is_train   = is_train\n","        self.data       = data\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","\n","        selected_data = self.data.iloc[idx]\n","\n","        cat_input = torch.tensor(selected_data[self.cat_col])                               # [batch size   x   # categorical features]\n","        num_input = torch.tensor(selected_data[self.num_col])                               # [batch size   x   # numerical features]\n","\n","        #TODO: Tokenize 의 결과가 자동으로 torch.Tensor 형태로 뽑힘. 이유는...모름\n","        sample              = selected_data['QandA']                                        # [batch size   x   sequence length]\n","        sample['cat_input'] = cat_input\n","        sample['num_input'] = num_input\n","\n","        # Include label only for training cases\n","        if self.is_train:\n","            label     = torch.tensor(selected_data[self.label_cols])                         # [batch size   x   1]\n","            sample['label'] = label\n","\n","        return sample\n","\n","    # ======================\n","    #    Helper Functions\n","    # ======================\n","\n","    def fix_grammar(self, answer: str) -> str:\n","        print(answer)\n","        answer = spell_checker.check(answer)\n","        return answer.checked\n","\n","    def fix_spacing(self, answer: str) -> str:\n","        answer  = answer.replace(\" \", '')\n","        spacing = Spacing()\n","        return spacing(answer)\n","\n","    def remove_punctuation(self, answer: str) -> str:\n","        answer = re.sub(r'[@%\\\\*=()/~#&\\+á?\\xc3\\xa1\\-\\|\\.\\:\\;\\!\\-\\,\\_\\~\\$\\'\\\"]', '', answer)\n","        answer = re.sub(r'\\s+', ' ', answer)        # remove extra space\n","        answer = re.sub(r\"^\\s+\", '', answer)        # remove space from start\n","        answer = re.sub(r'\\s+$', '', answer)        # remove space from the end\n","        return answer\n","\n","    def preprocess_txt(self, data: pd.DataFrame):\n","        data['Answer'] = data['Answer'].apply(self.fix_grammar)         #FIXME: 해당 패키지의 서버가 가끔 응답 오류가 남. 그럴 땐 주석 처리 필요. 데이터 저장해둘걸!\n","        data['Answer'] = data['Answer'].apply(self.fix_spacing)\n","        data['Answer'] = data['Answer'].apply(self.remove_punctuation)\n","\n","    def prepare_binary_classification(self, data: pd.DataFrame):\n","        one_list = ['E', 'N', 'F', 'P']\n","        zero_list = ['I', 'S', 'T', 'J']\n","\n","        for idx, mbti in enumerate(one_list):\n","            data[mbti] = data['MBTI'].str       \\\n","                .contains(mbti)                 \\\n","                .replace({True: 1, False: 0})\n","\n","            new_name = zero_list[idx] + '/' + mbti\n","            data.rename(columns = {mbti:new_name}, inplace=True)\n","\n","    def tokenize(self, data: pd.DataFrame):\n","\n","        def tokenize_per_sentence(series: pd.Series) -> str:\n","            selected_question = self.question_data.iloc[series['Q_number'] - 1].Question\n","            selected_answer = series['Answer']\n","\n","            # print('Q : ', selected_question)\n","            # print('A : ', selected_answer)\n","\n","            padding = False if self.padding_per_batch else 'longest'\n","            #TODO: 필요시 max_length 조절 필요\n","            return self.tokenizer(selected_question,\n","                                  selected_answer,\n","                                  padding=padding,\n","                                  return_tensors='pt')\n","\n","        data['QandA'] =  data.apply(tokenize_per_sentence, axis=1)\n"],"metadata":{"id":"sucudSchnHme","executionInfo":{"status":"ok","timestamp":1679495283387,"user_tz":-540,"elapsed":457,"user":{"displayName":"박상우","userId":"00596815453416531190"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["result = spell_checker.check(u'안녕 하세요. 저는 한국인 입니다. 이문장은 한글로 작성됬습니다.')\n","result"],"metadata":{"id":"5khGZ1ohusUp","executionInfo":{"status":"error","timestamp":1679495456107,"user_tz":-540,"elapsed":1579,"user":{"displayName":"박상우","userId":"00596815453416531190"}},"outputId":"20e57bc6-a107-4410-b0d2-35aac3ea0024","colab":{"base_uri":"https://localhost:8080/","height":321}},"execution_count":36,"outputs":[{"output_type":"error","ename":"JSONDecodeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m<ipython-input-36-94199f86bc9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspell_checker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'안녕 하세요. 저는 한국인 입니다. 이문장은 한글로 작성됬습니다.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/hanspell/spell_checker.py\u001b[0m in \u001b[0;36mcheck\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'result'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'html'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     result = {\n","\u001b[0;32m/usr/lib/python3.9/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.9/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.9/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"]}]},{"cell_type":"code","execution_count":35,"metadata":{"id":"mWlxq20EQjfj","colab":{"base_uri":"https://localhost:8080/","height":392},"executionInfo":{"status":"error","timestamp":1679495309470,"user_tz":-540,"elapsed":642,"user":{"displayName":"박상우","userId":"00596815453416531190"}},"outputId":"593c1238-6a77-44be-8fa3-67fa8114ceed"},"outputs":[{"output_type":"stream","name":"stdout","text":["<아니다> 어릴 때 왕따 당한 경험이 있고 외부 활동을 좋아하지 않기 때문에 소수의 친구와만 지냅니다.\n"]},{"output_type":"error","ename":"JSONDecodeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m<ipython-input-35-6efd45ccaebb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mquestion_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"data/question_filtered.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m data_III = IIIDataset(\n\u001b[0m\u001b[1;32m      5\u001b[0m    \u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m    \u001b[0mquestion_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-34-ece8420547dc>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_path, question_path, txt_preprocess, normalize, pretrained_url, padding_per_batch, is_binary_classification, is_bert, is_train)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;31m# preprocess data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtxt_preprocess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_txt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Age'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Age'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Age'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Age'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-34-ece8420547dc>\u001b[0m in \u001b[0;36mpreprocess_txt\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_txt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Answer'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Answer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfix_grammar\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m#FIXME: 해당 패키지의 서버가 가끔 응답 오류가 남. 그럴 땐 주석 처리 필요. 데이터 저장해둘걸!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Answer'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Answer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfix_spacing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Answer'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Answer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_punctuation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4431\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4432\u001b[0m         \"\"\"\n\u001b[0;32m-> 4433\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4435\u001b[0m     def _reduce(\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1086\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1088\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1089\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1141\u001b[0m                 \u001b[0;31m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m                 \u001b[0;31m# \"Callable[[Any], Any]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1143\u001b[0;31m                 mapped = lib.map_infer(\n\u001b[0m\u001b[1;32m   1144\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m                     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n","\u001b[0;32m<ipython-input-34-ece8420547dc>\u001b[0m in \u001b[0;36mfix_grammar\u001b[0;34m(self, answer)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfix_grammar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspell_checker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchecked\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/hanspell/spell_checker.py\u001b[0m in \u001b[0;36mcheck\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'result'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'html'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     result = {\n","\u001b[0;32m/usr/lib/python3.9/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.9/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.9/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"]}],"source":["data_path = \"data/example_train.csv\"\n","question_path = \"data/question_filtered.csv\"\n","\n","data_III = IIIDataset(\n","   data_path,\n","   question_path\n",")"]},{"cell_type":"code","source":["print(data_III.data.head())\n","print(len(data_III))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bZWZzrGXpXFH","executionInfo":{"status":"ok","timestamp":1679494397096,"user_tz":-540,"elapsed":13,"user":{"displayName":"박상우","userId":"00596815453416531190"}},"outputId":"97f101ff-e93d-4451-f9e6-7bb5e4463607"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["   Data_ID  User_ID  Gender       Age  MBTI  Q_number                                             Answer  I/E  S/N  T/F  J/P                                        QandA\n","0        1        1       1 -0.994778  INFP         1  <아니다> 어릴 때 왕따 당한 경험이 있고 외부 활동을 좋아하지 않기 때문에 소수의...    0    1    1    1  [input_ids, token_type_ids, attention_mask]\n","1        2        1       1 -0.994778  INFP         2  <중립> 다양한 관심사를 탐구하진 않지만 대체로 자연과 역사에 관련된 것을 좋아하며...    0    1    1    1  [input_ids, token_type_ids, attention_mask]\n","2        3        1       1 -0.994778  INFP         3  <그렇다> 감정이입이 잘 되어 코미디 영화에서 사람이 울고 있을 때도 울기 때문에 ...    0    1    1    1  [input_ids, token_type_ids, attention_mask]\n","3        4        1       1 -0.994778  INFP         4  <중립> 대비책을 세우긴 하는데 세우다가 마는 편입니다일의 변수가 생길 수 있고 변...    0    1    1    1  [input_ids, token_type_ids, attention_mask]\n","4        5        1       1 -0.994778  INFP         5  <아니다> 평정심을 유지 못하는 편입니다 머릿속은 백지화가 된 상태로 말도 제대로 ...    0    1    1    1  [input_ids, token_type_ids, attention_mask]\n","96\n"]}]},{"cell_type":"code","source":["# Tokenize 의 결과가 tf 로 나오는지 확인\n","sample = data_III.data.iloc[0]['QandA']\n","print(type(sample['input_ids']))\n","print(type(sample['token_type_ids']))\n","print(type(sample['attention_mask']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OwbfS4EZqUu0","executionInfo":{"status":"ok","timestamp":1679494527647,"user_tz":-540,"elapsed":299,"user":{"displayName":"박상우","userId":"00596815453416531190"}},"outputId":"ee08bb12-e571-4abb-bc69-e3b162780d92"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'torch.Tensor'>\n","<class 'torch.Tensor'>\n","<class 'torch.Tensor'>\n"]}]},{"cell_type":"code","source":["# pd.DataFrame 도 받아들이는지 확인 -> 잘 나온다\n","df = data_III.data\n","\n","data_IIII = IIIDataset(\n","   df,\n","   question_path\n",")"],"metadata":{"id":"e5a2KEP7nzXX","executionInfo":{"status":"ok","timestamp":1679494697397,"user_tz":-540,"elapsed":2,"user":{"displayName":"박상우","userId":"00596815453416531190"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["print(data_IIII.data.head())\n","print(len(data_IIII))\n","\n","# 이런 식으로 4개의 MBTI column 을 뽑은 후에 4개의 DataLoader 을 만들면 될 듯"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2ige-BKTq7zu","executionInfo":{"status":"ok","timestamp":1679494698278,"user_tz":-540,"elapsed":3,"user":{"displayName":"박상우","userId":"00596815453416531190"}},"outputId":"403d7848-ce03-4c8f-a00a-74680c796fc9"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["   Data_ID  User_ID  Gender       Age  MBTI  Q_number                                             Answer  I/E  S/N  T/F  J/P                                        QandA\n","0        1        1       1 -0.994778  INFP         1  <아니다> 어릴 때 왕따 당한 경험이 있고 외부 활동을 좋아하지 않기 때문에 소수의...    0    1    1    1  [input_ids, token_type_ids, attention_mask]\n","1        2        1       1 -0.994778  INFP         2  <중립> 다양한 관심사를 탐구하진 않지만 대체로 자연과 역사에 관련된 것을 좋아하며...    0    1    1    1  [input_ids, token_type_ids, attention_mask]\n","2        3        1       1 -0.994778  INFP         3  <그렇다> 감정이입이 잘 되어 코미디 영화에서 사람이 울고 있을 때도 울기 때문에 ...    0    1    1    1  [input_ids, token_type_ids, attention_mask]\n","3        4        1       1 -0.994778  INFP         4  <중립> 대비책을 세우긴 하는데 세우다가 마는 편입니다일의 변수가 생길 수 있고 변...    0    1    1    1  [input_ids, token_type_ids, attention_mask]\n","4        5        1       1 -0.994778  INFP         5  <아니다> 평정심을 유지 못하는 편입니다 머릿속은 백지화가 된 상태로 말도 제대로 ...    0    1    1    1  [input_ids, token_type_ids, attention_mask]\n","96\n"]}]},{"cell_type":"code","source":["# test_data 에도 확인 -> 되는 듯!\n","test_path = \"data/hackathon_test_for_user.csv\"\n","\n","data_IIII = IIIDataset(\n","   test_path,\n","   question_path\n",")"],"metadata":{"id":"bscTkQEdrmEc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(data_IIII.data.head())\n","print(len(data_IIII))"],"metadata":{"id":"BsLeKrzAshbU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Xw7OznT1skZ9"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"1Zo9ysYNbKvkGLQSftjh95wENxJfOomoC","authorship_tag":"ABX9TyONP8brzICFCfTamhkykbLN"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}