{"cells":[{"cell_type":"markdown","metadata":{"id":"6cU4hofynAuo"},"source":["# Set up environment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6BofWjRAmbcc"},"outputs":[],"source":["# Uncomment below section and run in case of re-connecting Colab\n","\n","!pip install git+https://github.com/haven-jeon/PyKoSpacing.git\n","!pip install transformers\n","!pip install git+https://github.com/ssut/py-hanspell.git\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# 영도야 여긴 너 경로에 맞게 바꿔야 할거야\n","%cd drive/MyDrive/MBTI\n","!pwd"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"DyiXeOjimgi9","executionInfo":{"status":"ok","timestamp":1680498804220,"user_tz":-540,"elapsed":19293,"user":{"displayName":"박상우","userId":"00596815453416531190"}}},"outputs":[],"source":["import time\n","import datetime\n","import random\n","from tqdm import tqdm\n","\n","from dataloader import MBTIDataset\n","\n","import pandas as pd\n","pd.set_option('display.width', 180)\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n","from sklearn.model_selection import KFold\n","from transformers import DataCollatorWithPadding, BertForSequenceClassification, BertConfig, AdamW"]},{"cell_type":"markdown","metadata":{"id":"ARc_jwe5nF8g"},"source":["# Environment"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"Ef5Z75ozmW8m","executionInfo":{"status":"ok","timestamp":1680498804220,"user_tz":-540,"elapsed":21,"user":{"displayName":"박상우","userId":"00596815453416531190"}}},"outputs":[],"source":["# Setup\n","env_dict = {\n","    # ==== Arguments for dataset =====\n","    'train_path'        : './data/hackathon_train_v1.csv',\n","    'question_path'     : './data/question_filtered.csv',\n","    'pretrained_url'    : \"klue/bert-base\",\n","    'padding_per_batch' : True,\n","    # ==== Arguments for dataloader =====\n","    'shuffle'           : False,            # turn off 'shuffle' since we use sampler in Dataloader\n","    # ==== Arguments for training =====\n","    'target'            : 'I/E',\n","    'lm'                : 'bert',\n","    'classifier'        : 'mlp',\n","    'batch_size'        : 64,\n","    'epoch'             : 5,\n","    'lr'                : 3e-3,\n","    'decay_rate'        : 1e-7,\n","    'dropout'           : 0.1,\n","    'hidden_dim'        : [192, 48, 12]     # 일단 설정해둔 숫자들 (cls token의 dimension인 768 을 4로 나눈 값들)\n","}\n","\n","# Random seed\n","seed_val = 1234\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)"]},{"cell_type":"code","source":["# 지워질 Cell!!!!!\n","import os\n","import re\n","import time\n","import datetime\n","import pickle\n","from tqdm import tqdm\n","from typing import Union\n","\n","import torch\n","import pandas as pd\n","from torch.utils.data import Dataset\n","from pykospacing import Spacing\n","from hanspell import spell_checker\n","from transformers import AutoTokenizer\n","\n","#TODO:\n","# 1. train / text 다른 로직이 필요 (데이터 형식이 조금 다름)\n","# 2. 다른 Text preprocess 방식 도입 검토\n","#       ref : https://ebbnflow.tistory.com/246\n","\n","class MBTIIDataset(Dataset):\n","    def __init__(\n","        self,\n","        data_path     : Union[str, pd.DataFrame],\n","        question_path : Union[str, pd.DataFrame],\n","        txt_preprocess: bool            = True,\n","        normalize     : bool            = True,\n","        pretrained_url: str             = \"klue/bert-base\",\n","        padding_per_batch               = True,\n","        is_binary_classification: bool  = True,\n","        is_bert       : bool            = True,\n","        is_train      : bool            = True\n","        ):\n","        \"\"\"DataLoader for MBTI dataset\n","\n","        Args:\n","            data_path (str): Data file path. Both csv and parguet files are allowed.\n","            question_path (str): Question file path. Both csv and parguet files are allowed.\n","            txt_preprocess (bool, optional): Text preprocessing pipeline. (e.g. fixing grammar, removing punctuations). Defaults to True.\n","            normalize (bool, optional): Normalize numeric attribute. Defaults to True.\n","            is_binary_classification (bool, optional): Target of task. You can choose btw Multi-class classificaiton\n","                and 4 binary classification problem. Defaults to True.\n","            is_bert (bool, optional): Using BERT for language model or not. Defaults to True.\n","            is_train (bool, optional): Whether given data is for training or not. Defaults to True.\n","        \"\"\"\n","        \n","        def resolve_path(path:str)->pd.DataFrame:\n","            if path.endswith('.csv'):\n","                try:\n","                    df = pd.read_csv(path)\n","                except:\n","                    df = pd.read_csv(path, encoding='cp949')\n","            else:\n","                df = pd.read_parquet(path)\n","            return df\n","\n","        data = None\n","        question_data = None\n","        label_cols = ['I/E', 'S/N', 'T/F', 'J/P']\n","        # if given data_path is pd.Dataframe, we assume preprocessing is already applied to given Dataframe\n","        # so that it can skip all the processes below\n","        if not isinstance(data_path, pd.DataFrame):\n","            data = resolve_path(data_path)\n","            question_data = resolve_path(question_path)\n","\n","            self.question_data = question_data\n","\n","            # preprocess data\n","            if txt_preprocess:\n","                self.preprocess_txt(data)\n","            if normalize:\n","                data['Age'] = (data['Age'] - data['Age'].mean()) / data['Age'].std()\n","\n","            # make dataset suitable for binary classification (only for training data - test data doesn't contain 'MBTI' field)\n","            if is_train and is_binary_classification:\n","                self.prepare_binary_classification(data)\n","                # if method right above works successfully, then data should contain same # 0 and 1.\n","                for col in label_cols:\n","                    value_counted = data[col].value_counts()\n","                    assert value_counted[0] == value_counted[1]\n","\n","            # tokenize\n","            self.tokenizer = AutoTokenizer.from_pretrained(pretrained_url)\n","            self.padding_per_batch = padding_per_batch\n","            self.tokenize(data)\n","\n","        else:\n","            data = data_path\n","\n","        # set columns for both training and inference\n","        #TODO: 테스트 데이터를 고려해서 유저 정보를 학습에 활용하지 않는 상황. 필요시 고쳐야 함\n","        self.cat_col    = ['Gender']\n","        self.num_col    = ['Age']\n","        self.label_cols = label_cols\n","        self.is_train   = is_train\n","        self.data       = data\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","\n","        selected_data = self.data.iloc[idx]\n","\n","        cat_input = torch.tensor(selected_data[self.cat_col])                               # [batch size   x   # categorical features]\n","        num_input = torch.tensor(selected_data[self.num_col])                               # [batch size   x   # numerical features]\n","\n","        sample              = selected_data['QandA']                                        # [batch size   x   sequence length]\n","        sample['cat_input'] = cat_input\n","        sample['num_input'] = num_input\n","\n","        # Include label only for training cases\n","        if self.is_train:\n","            for col in self.label_cols:\n","              label = torch.tensor(selected_data[col])                            # [batch size   x   1]\n","              sample[col] = label\n","\n","        return sample\n","\n","    # ======================\n","    #    Helper Functions\n","    # ======================\n","\n","    def fix_grammar(self, answer: str) -> str:\n","        answer = spell_checker.check(answer)\n","        return answer.checked\n","\n","    def fix_spacing(self, answer: str) -> str:\n","        answer  = answer.replace(\" \", '')\n","        spacing = Spacing()\n","        return spacing(answer)\n","\n","    def remove_punctuation(self, answer: str) -> str:\n","        answer = re.sub(r'[@%\\\\*=()/~#&\\+á?\\xc3\\xa1\\-\\|\\.\\:\\;\\!\\-\\,\\_\\~\\$\\'\\\"]', '', answer)\n","        answer = re.sub(r'\\s+', ' ', answer)        # remove extra space\n","        answer = re.sub(r\"^\\s+\", '', answer)        # remove space from start\n","        answer = re.sub(r'\\s+$', '', answer)        # remove space from the end\n","        return answer\n","\n","    def preprocess_txt(self, data: pd.DataFrame):\n","        try:\n","            print('=============== fix_grammar ===============')\n","            data['Answer'] = data['Answer'].apply(self.fix_grammar)         #FIXME: 해당 패키지의 서버가 가끔 응답 오류가 남...\n","        except:\n","            pass\n","        tqdm.pandas()\n","        print('=============== fix_spacing ===============')\n","        data['Answer'] = data['Answer'].progress_apply(self.fix_spacing)\n","        print('=============== remove_punctuation ===============')\n","        data['Answer'] = data['Answer'].progress_apply(self.remove_punctuation)\n","\n","    def prepare_binary_classification(self, data: pd.DataFrame):\n","        one_list = ['E', 'N', 'F', 'P']\n","        zero_list = ['I', 'S', 'T', 'J']\n","\n","        print('=============== prepare_binary_classification ===============')\n","        for idx, mbti in tqdm(enumerate(one_list)):\n","            data[mbti] = data['MBTI'].str               \\\n","                .contains(mbti)                         \\\n","                .replace({True: 1, False: 0})\n","\n","            new_name = zero_list[idx] + '/' + mbti\n","            data.rename(columns = {mbti:new_name}, inplace=True)\n","\n","    def tokenize(self, data: pd.DataFrame):\n","        \n","        #TODO: tokenizing tqdm 적용해두기\n","        def tokenize_per_sentence(series: pd.Series) -> str:\n","            selected_question = self.question_data.iloc[series['Q_number'] - 1].Question\n","            selected_answer = series['Answer']\n","\n","            padding = False if self.padding_per_batch else 'longest'\n","            #TODO: 필요시 max_length 조절 필요\n","            return self.tokenizer(selected_question,\n","                                  selected_answer,\n","                                  padding=padding)\n","\n","        tqdm.pandas()\n","        print('=============== tokenize ===============')\n","        data['QandA'] =  data.progress_apply(tokenize_per_sentence, axis=1)\n"],"metadata":{"id":"6NMyiI_B3LRs","executionInfo":{"status":"ok","timestamp":1680498855152,"user_tz":-540,"elapsed":2,"user":{"displayName":"박상우","userId":"00596815453416531190"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Garbage collect\n","import torch, gc\n","gc.collect()\n","torch.cuda.empty_cache()"],"metadata":{"id":"EXNJJ7ZpWDwI","executionInfo":{"status":"ok","timestamp":1680498809380,"user_tz":-540,"elapsed":3,"user":{"displayName":"박상우","userId":"00596815453416531190"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# Prepare dataset"],"metadata":{"id":"OmT20qIS3CfU"}},{"cell_type":"code","execution_count":18,"metadata":{"id":"MzufA7B9mW8o","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680499653644,"user_tz":-540,"elapsed":615983,"user":{"displayName":"박상우","userId":"00596815453416531190"}},"outputId":"b5003c42-20f6-45d6-ebb7-e9cb78278356"},"outputs":[{"output_type":"stream","name":"stdout","text":["=============== fix_spacing ===============\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 11520/11520 [10:07<00:00, 18.97it/s]\n"]},{"output_type":"stream","name":"stdout","text":["=============== remove_punctuation ===============\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 11520/11520 [00:00<00:00, 31081.21it/s]\n"]},{"output_type":"stream","name":"stdout","text":["=============== prepare_binary_classification ===============\n"]},{"output_type":"stream","name":"stderr","text":["4it [00:00, 65.24it/s]\n"]},{"output_type":"stream","name":"stdout","text":["=============== tokenize ===============\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 11520/11520 [00:06<00:00, 1737.02it/s]"]},{"output_type":"stream","name":"stdout","text":["11520\n","   Data_ID  User_ID  Gender       Age  MBTI  Q_number                                             Answer  I/E  S/N  T/F  J/P                                        QandA\n","0        1        1       1 -0.372581  INFP         1  <아니다> 어릴 때 왕따 당한 경험이 있고 외부 활동을 좋아하지 않기 때문에 소수의...    0    1    1    1  [input_ids, token_type_ids, attention_mask]\n","1        2        1       1 -0.372581  INFP         2  <중립> 다양한 관심사를 탐구하진 않지만 대체로 자연과 역사에 관련된 것을 좋아하며...    0    1    1    1  [input_ids, token_type_ids, attention_mask]\n","2        3        1       1 -0.372581  INFP         3  <그렇다> 감정이입이 잘 되어 코미디 영화에서 사람이 울고 있을 때도 울기 때문에 ...    0    1    1    1  [input_ids, token_type_ids, attention_mask]\n","3        4        1       1 -0.372581  INFP         4  <중립> 대비책을 세우긴 하는데 세우다가 마는 편입니다일의 변수가 생길 수 있고 변...    0    1    1    1  [input_ids, token_type_ids, attention_mask]\n","4        5        1       1 -0.372581  INFP         5  <아니다> 평정심을 유지 못하는 편입니다 머릿속은 백지화가 된 상태로 말도 제대로 ...    0    1    1    1  [input_ids, token_type_ids, attention_mask]\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["# Uncomment & run this cell only if there is no preprocessed data\n","\n","# Dataset\n","# train_dataset = MBTIIDataset(\n","#     data_path           = env_dict['train_path'],\n","#     question_path       = env_dict['question_path'],\n","#     pretrained_url      = env_dict['pretrained_url'],\n","#     padding_per_batch   = env_dict['padding_per_batch'],\n","#     txt_preprocess      = True,\n","#     is_train            = True\n","# )\n","\n","# print(len(train_dataset))\n","# print(train_dataset.data.head())"]},{"cell_type":"code","source":["# Uncomment & run this cell only if there is no preprocessed data\n","\n","# # save preprocessed pd.Dataframe & tokenizer as pickle data format\n","\n","data_path = './data/' + 'base_data_2.pickle'\n","# with open(data_path, 'wb') as handle:\n","#     pickle.dump(train_dataset.data, handle)\n","\n","# with open('./data/base_tokenizer.pickle', 'wb') as handle:\n","#     pickle.dump(train_dataset.tokenizer, handle)"],"metadata":{"id":"r99ktXNBztto","executionInfo":{"status":"ok","timestamp":1680506949998,"user_tz":-540,"elapsed":4,"user":{"displayName":"박상우","userId":"00596815453416531190"}}},"execution_count":59,"outputs":[]},{"cell_type":"code","source":["# Load data as pd.DataFrame & tokenizer\n","\n","with open(data_path, 'rb') as handle:\n","    df = pickle.load(handle)\n","\n","with open('./data/base_tokenizer.pickle', 'rb') as handle:\n","    tokenizer = pickle.load(handle)\n","\n","train_dataset = MBTIIDataset(\n","    data_path           = df,\n","    question_path       = env_dict['question_path'],\n","    pretrained_url      = env_dict['pretrained_url'],\n","    padding_per_batch   = env_dict['padding_per_batch'],\n","    is_train            = True\n",")\n","\n","print(len(train_dataset))\n","print(train_dataset.data.head())"],"metadata":{"id":"kN0d5eTQWLNV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680506953229,"user_tz":-540,"elapsed":1108,"user":{"displayName":"박상우","userId":"00596815453416531190"}},"outputId":"5e61048f-3656-4727-f43e-3b887113ed9f"},"execution_count":60,"outputs":[{"output_type":"stream","name":"stdout","text":["11520\n","   Data_ID  User_ID  Gender       Age  MBTI  Q_number                                             Answer  I/E  S/N  T/F  J/P                                              QandA\n","0        1        1       1 -0.372581  INFP         1  <아니다> 어릴 때 왕따 당한 경험이 있고 외부 활동을 좋아하지 않기 때문에 소수의...    0    1    1    1  [input_ids, token_type_ids, attention_mask, ca...\n","1        2        1       1 -0.372581  INFP         2  <중립> 다양한 관심사를 탐구하진 않지만 대체로 자연과 역사에 관련된 것을 좋아하며...    0    1    1    1  [input_ids, token_type_ids, attention_mask, ca...\n","2        3        1       1 -0.372581  INFP         3  <그렇다> 감정이입이 잘 되어 코미디 영화에서 사람이 울고 있을 때도 울기 때문에 ...    0    1    1    1  [input_ids, token_type_ids, attention_mask, ca...\n","3        4        1       1 -0.372581  INFP         4  <중립> 대비책을 세우긴 하는데 세우다가 마는 편입니다일의 변수가 생길 수 있고 변...    0    1    1    1  [input_ids, token_type_ids, attention_mask, ca...\n","4        5        1       1 -0.372581  INFP         5  <아니다> 평정심을 유지 못하는 편입니다 머릿속은 백지화가 된 상태로 말도 제대로 ...    0    1    1    1  [input_ids, token_type_ids, attention_mask, ca...\n"]}]},{"cell_type":"code","execution_count":62,"metadata":{"id":"wBo4Xtc6oCls","executionInfo":{"status":"ok","timestamp":1680507020107,"user_tz":-540,"elapsed":3,"user":{"displayName":"박상우","userId":"00596815453416531190"}}},"outputs":[],"source":["# define collator function when padding per batch is needed\n","#TODO: data_collator가 아닌 torch의 Packing 을 이용하는 것과 성능 비교가 필요함\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer) if env_dict['padding_per_batch'] else None\n","\n","# Dataloader\n","train_dataloader = DataLoader(\n","    train_dataset,\n","    batch_size  = env_dict['batch_size'],\n","    shuffle     = env_dict['shuffle'],\n","    collate_fn  = data_collator\n",")"]},{"cell_type":"code","source":["# Example result from dataloader\n","next(iter(train_dataloader))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NFvIVkJqBinP","executionInfo":{"status":"ok","timestamp":1680507041998,"user_tz":-540,"elapsed":2,"user":{"displayName":"박상우","userId":"00596815453416531190"}},"outputId":"5eb88669-97ee-47c8-d9b3-977ed2b89a65"},"execution_count":63,"outputs":[{"output_type":"stream","name":"stderr","text":["You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"output_type":"execute_result","data":{"text/plain":["{'input_ids': tensor([[    2,  7267, 11187,  ...,     0,     0,     0],\n","        [    2,  3936,  3641,  ...,     0,     0,     0],\n","        [    2,  3656,  3611,  ...,     0,     0,     0],\n","        ...,\n","        [    2,  4051,  4362,  ...,     0,     0,     0],\n","        [    2,  3656,  3611,  ...,     0,     0,     0],\n","        [    2,  3971,  3746,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        ...,\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]]), 'cat_input': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'num_input': tensor([-0.3726, -0.3726, -0.3726, -0.3726, -0.3726, -0.3726, -0.3726, -0.3726,\n","        -0.3726, -0.3726, -0.3726, -0.3726, -0.3726, -0.3726, -0.3726, -0.3726,\n","        -0.3726, -0.3726, -0.3726, -0.3726, -0.3726, -0.3726, -0.3726, -0.3726,\n","        -0.3726, -0.3726, -0.3726, -0.3726, -0.3726, -0.3726, -0.3726, -0.3726,\n","        -0.3726, -0.3726, -0.3726, -0.3726, -0.3726, -0.3726, -0.3726, -0.3726,\n","        -0.3726, -0.3726, -0.3726, -0.3726, -0.3726, -0.3726, -0.3726, -0.3726,\n","         1.0246,  1.0246,  1.0246,  1.0246,  1.0246,  1.0246,  1.0246,  1.0246,\n","         1.0246,  1.0246,  1.0246,  1.0246,  1.0246,  1.0246,  1.0246,  1.0246],\n","       dtype=torch.float64), 'I/E': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'S/N': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'T/F': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'J/P': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}"]},"metadata":{},"execution_count":63}]},{"cell_type":"markdown","metadata":{"id":"60pvecWkpznO"},"source":["# Prepare language model"]},{"cell_type":"code","execution_count":64,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1680507080439,"user":{"displayName":"박상우","userId":"00596815453416531190"},"user_tz":-540},"id":"tbVO_n8BQ1bc","outputId":"52d90cee-b044-4e8e-c2f6-cadbc566bef5"},"outputs":[{"output_type":"stream","name":"stdout","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla T4\n"]}],"source":["# GPU preparation\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","else:\n","    device = torch.device(\"cpu\")\n","    print('No GPU available, using CPU instead.')"]},{"cell_type":"code","source":["from mlp import MLPClassifier\n","\n","class BertWithMlp(BertForSequenceClassification):\n","    def __init__(\n","        self,\n","        config,\n","        input_dim = None,\n","        hidden_dim = None,\n","        num_classes = 2,\n","        dropout = 0.1\n","        ):\n","\n","        # ====================\n","        #      BERT Setup\n","        # ====================\n","\n","        # resulting BERT model is stored in 'self.bert'.\n","        super().__init__(config)\n","\n","        self.num_labels = config.num_labels\n","\n","        # 나중에 config 내부에 해당 field 값을 넣어주면 됨 (영상 참고)\n","        combined_feat_dim = config.text_feat_dim + config.cat_feat_dim + config.num_feat_dim\n","        print(\"combined_feat_dim :\", combined_feat_dim)\n","\n","        self.mlp = MLPClassifier(\n","            combined_feat_dim,\n","            None,\n","            hidden_dim,\n","            num_classes=num_classes,\n","            dropout=dropout\n","        )\n","        self.dropout = nn.Dropout(p=dropout, inplace=False)\n","        self.bn = nn.BatchNorm1d(config.num_feat_dim)\n","\n","    def forward(\n","        self,\n","        input_ids = None,\n","        attention_mask = None,\n","        token_type_ids = None,\n","        position_ids = None,\n","        head_mask = None,\n","        inputs_embeds = None,\n","        labels = None,\n","        output_attentions = None,\n","        cat_feats = None,\n","        num_feats = None\n","    ):\n","        # ====================\n","        #     BERT forward\n","        # ====================\n","        #TODO: 더 많은 인자 추가해주기\n","        logits = self.bert(\n","            input_ids,\n","            token_type_ids=token_type_ids,\n","            attention_mask=attention_mask)\n","\n","        cls = logits[1]\n","        print(\"logits :\", logits[0].shape, logits[1].shape)\n","        # Apply dropout to cls\n","        cls = self.dropout(cls)\n","        # Apply batch normalization to numerical features\n","        # num_feats = self.bn(num_feats)\n","\n","        print(\"cls shape :\", cls.shape)\n","        print(\"cat shape :\", cat_feats.shape)\n","        print(\"num shape :\", num_feats.shape)\n","\n","        # ====================\n","        #      MLP forward\n","        # ====================\n","        all_feats = torch.cat((cls, cat_feats.view(-1, 1), num_feats.view(-1, 1)), dim=1)\n","        output = self.mlp(all_feats)\n","\n","        return output\n"],"metadata":{"id":"ZpVnMOF-9yDs","executionInfo":{"status":"ok","timestamp":1680507342922,"user_tz":-540,"elapsed":536,"user":{"displayName":"박상우","userId":"00596815453416531190"}}},"execution_count":81,"outputs":[]},{"cell_type":"code","source":["# Update config file\n","from transformers import BertConfig\n","\n","#TODO: num_labels 인자가 필요한지 알아봐야 함\n","config = BertConfig.from_pretrained(\n","    env_dict['pretrained_url'],\n","    num_labels = 2\n","  )\n","\n","config.num_feat_dim = 1\n","config.cat_feat_dim = 1\n","config.text_feat_dim = config.hidden_size\n","\n","print(config)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wHzf_UILKiZW","executionInfo":{"status":"ok","timestamp":1680507345973,"user_tz":-540,"elapsed":828,"user":{"displayName":"박상우","userId":"00596815453416531190"}},"outputId":"d621f757-4373-4173-dd13-860a557a2035"},"execution_count":82,"outputs":[{"output_type":"stream","name":"stdout","text":["BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"cat_feat_dim\": 1,\n","  \"classifier_dropout\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_feat_dim\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"text_feat_dim\": 768,\n","  \"transformers_version\": \"4.27.4\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 32000\n","}\n","\n"]}]},{"cell_type":"code","source":["# Prepare model\n","model = BertWithMlp.from_pretrained(\n","    env_dict['pretrained_url'],\n","    config      = config,\n","    hidden_dim  = env_dict['hidden_dim'],\n","    dropout     = env_dict['dropout']\n","    )\n","model.cuda()\n","\n","# Apply weight decaying except for bias & layer normalization term\n","no_decay = ['bias', 'LayerNorm.weight']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","]\n","\n","# Prepare optimizer\n","optimizer = AdamW(optimizer_grouped_parameters, lr=env_dict['lr'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QwILgWVI-ma5","executionInfo":{"status":"ok","timestamp":1680507349500,"user_tz":-540,"elapsed":2834,"user":{"displayName":"박상우","userId":"00596815453416531190"}},"outputId":"406dfe8e-1d74-4d8d-dae6-57a2949cd2ba"},"execution_count":83,"outputs":[{"output_type":"stream","name":"stdout","text":["combined_feat_dim : 770\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at klue/bert-base were not used when initializing BertWithMlp: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertWithMlp from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertWithMlp from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertWithMlp were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['mlp.layers.3.bias', 'mlp.layers.0.bias', 'bn.bias', 'classifier.bias', 'bn.running_var', 'mlp.layers.3.weight', 'classifier.weight', 'bn.num_batches_tracked', 'mlp.layers.1.bias', 'mlp.layers.2.weight', 'mlp.layers.0.weight', 'bn.weight', 'bn.running_mean', 'mlp.layers.1.weight', 'mlp.layers.2.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["# Freeze Encoder, use head's parameters only\n","free_encoder = True\n","\n","if free_encoder:\n","  for param in model.base_model.parameters():\n","    param.requires_grad = False"],"metadata":{"id":"aSqmFz0aiSph","executionInfo":{"status":"ok","timestamp":1680507349501,"user_tz":-540,"elapsed":4,"user":{"displayName":"박상우","userId":"00596815453416531190"}}},"execution_count":84,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xsg8RoRjXjQE"},"source":["# Train"]},{"cell_type":"code","source":["def format_time(elapsed):\n","    elapsed_rounded = int(round((elapsed)))\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"],"metadata":{"id":"5LN0_mWU8Yvj","executionInfo":{"status":"ok","timestamp":1680507212022,"user_tz":-540,"elapsed":3,"user":{"displayName":"박상우","userId":"00596815453416531190"}}},"execution_count":70,"outputs":[]},{"cell_type":"code","source":["# Set the target of training right before training loop\n","env_dict['target_of_training'] = 'I/E'"],"metadata":{"id":"9Z6v5b5ukwBb","executionInfo":{"status":"ok","timestamp":1680507212563,"user_tz":-540,"elapsed":1,"user":{"displayName":"박상우","userId":"00596815453416531190"}}},"execution_count":71,"outputs":[]},{"cell_type":"code","source":["# Training Loop for 영도\n","model.train()\n","\n","for epoch in range(env_dict['epoch']):\n","  t0 = time.time()\n","  epoch_loss = 0\n","  for step, batch in tqdm(enumerate(train_dataloader)):\n","\n","    input_ids       = batch['input_ids'].to(device)\n","    token_type_ids  = batch['token_type_ids'].to(device)\n","    attention_mask  = batch['attention_mask'].to(device)\n","    cat_input       = batch['cat_input'].to(device)\n","    num_input       = batch['num_input'].to(device)\n","    labels  = batch[env_dict['target_of_training']].to(device)\n","\n","    # Clear prior gradients\n","    model.zero_grad()\n","\n","    # Forward\n","    output = model(input_ids,\n","                    token_type_ids=token_type_ids,\n","                    attention_mask=attention_mask,\n","                    cat_feats=cat_input,\n","                    num_feats=num_input)\n","\n","    # Calculate loss - CELoss 함수 짜서 실험 좀 부탁해유~\n","\n","    print(output)\n","\n","    break\n","\n","\n","    print(\"Step loss: {0:.2f}\".format(loss))\n","\n","    loss.backward()\n","    optimizer.step()\n","\n","  avg_train_loss = total_loss / len(train_dataloader)\n","  print(\"Epoch loss: {0:.2f}\".format(loss))\n","  print(\"Training epoch took: {:}\".format(format_time(time.time() - t0)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":470},"id":"_2U999FINgOv","executionInfo":{"status":"error","timestamp":1680507350554,"user_tz":-540,"elapsed":5,"user":{"displayName":"박상우","userId":"00596815453416531190"}},"outputId":"e56c9af7-adec-4c25-bf44-6528f4a1c652"},"execution_count":85,"outputs":[{"output_type":"stream","name":"stderr","text":["0it [00:00, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["logits : torch.Size([64, 104, 768]) torch.Size([64, 768])\n","cls shape : torch.Size([64, 768])\n","cat shape : torch.Size([64])\n","num shape : torch.Size([64])\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-85-888d5b68782c>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     output = model(input_ids,\n\u001b[0m\u001b[1;32m     21\u001b[0m                     \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-81-0b2c508c1c1d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, cat_feats, num_feats)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# ====================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mall_feats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat_feats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_feats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_feats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/MBTI/mlp.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;31m# If layer is not the last layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype"]}]},{"cell_type":"markdown","metadata":{"id":"1LUMI4BiZVac"},"source":["# Save trained model"]},{"cell_type":"code","source":["# Save\n","#TODO: haperparams가 이름에 드러날 수 있는 저장경로 생각해보기\n","save_path = './models/' + env_dict['lm'] + 'with' + env_dict['classifier'] + '.pt'\n","torch.save(model.state_dict(), save_path)"],"metadata":{"id":"_rnIRUdoNfSo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 저장된 모델을 다른 파일에 불러와서 Test 하길 권장! (test.py 만들어도 좋아)"],"metadata":{"id":"RGy7ri7aRjed"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# K-Fold Cross Validation (수정 중)"],"metadata":{"id":"4iVUGgx9NXeT"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"R-DVssQfmW8p"},"outputs":[],"source":["# Training with cross validation (ref : https://velog.io/@pppanghyun/6.-%EA%B5%90%EC%B0%A8-%EA%B2%80%EC%A6%9DCross-Validation)\n","#TODO: Scheduler, Gradient Clipping\n","\n","kfold     = KFold(n_splits=5, shuffle=True)\n","criterion = torch.nn.MSELoss()\n","\n","validation_loss = []\n","\n","for fold, (train_idx, val_idx) in enumerate(kfold.split(train_dataset)):\n","\n","    #TODO: Fold 끼리 겹칠 수 있음. 겹치지 않는 방식 고려 필요\n","    # Make indices for both training and validation\n","    train_subsampler  = SubsetRandomSampler(train_idx)\n","    val_subsampler    = SubsetRandomSampler(val_idx)\n","\n","    # Define dataloader using sampler\n","    train_dataloder = DataLoader(\n","        train_dataset,\n","        batch_size  = env_dict['batch_size'],\n","        shuffle     = env_dict['shuffle'],\n","        sampler     = train_subsampler,\n","        collate_fn  = data_collator\n","    )\n","    val_dataloder   = DataLoader(\n","        train_dataset,\n","        batch_size  = env_dict['batch_size'],\n","        shuffle     = env_dict['shuffle'],\n","        sampler     = val_subsampler,\n","        collate_fn  = data_collator\n","    )\n","\n","    # ===================\n","    #    Training Loop\n","    # ===================\n","\n","    optimizer = AdamW(model.parameters(), lr=env_dict['lr'], weight_decay=env_dict['decay_rate'])\n","\n","    model.train()\n","\n","    for epoch in range(env_dict['epoch']):\n","      t0 = time.time()\n","      epoch_loss = 0\n","      for step, batch in tqdm(enumerate(train_dataloader)):\n","\n","        input_ids       = batch['input_ids'].to(device)\n","        token_type_ids  = batch['token_type_ids'].to(device)\n","        attention_mask  = batch['attention_mask'].to(device)\n","        labels  = batch[env_dict['target_of_training']].to(device)\n","\n","        # Clear prior gradients\n","        model.zero_grad()\n","\n","        # Forward\n","        output = model(input_ids,\n","                       token_type_ids=token_type_ids,\n","                       attention_mask=attention_mask,\n","                       cat_feats=batch['cat_input'],\n","                       num_feats=batch['num_input'])\n","\n","        # Calculate loss\n","\n","        # loss    = outputs.loss     # Default : CELoss\n","\n","        print(\"Step loss: {0:.2f}\".format(loss))\n","\n","        loss.backward()\n","        optimizer.step()\n","      \n","      avg_train_loss = total_loss / len(train_dataloader)\n","      print(\"Epoch loss: {0:.2f}\".format(loss))\n","      print(\"Training epoch took: {:}\".format(format_time(time.time() - t0)))\n","\n","#     train_rmse = evaluation(trainloader) # 학습 데이터의 RMSE\n","#     val_rmse = evaluation(valloader)\n","#     print(\"k-fold\", fold,\" Train Loss: %.4f, Validation Loss: %.4f\" %(train_rmse, val_rmse)) \n","#     validation_loss.append(val_rmse)\n","\n","## Calculate validation score\n","\n","# validation_loss = np.array(validation_loss)\n","# mean = np.mean(validation_loss)\n","# std = np.std(validation_loss)\n","# print(\"Validation Score: %.4f, ± %.4f\" %(mean, std))"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"toc_visible":true},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.9.7 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"7337906654535f92ad925780206611836a0922d3540b9d89ec64204ed4b58f9b"}}},"nbformat":4,"nbformat_minor":0}