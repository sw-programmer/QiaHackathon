{"cells":[{"cell_type":"markdown","metadata":{"id":"6cU4hofynAuo"},"source":["# Set up environment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6BofWjRAmbcc"},"outputs":[],"source":["# Uncomment below section and run in case of re-connecting Colab\n","\n","!pip install git+https://github.com/haven-jeon/PyKoSpacing.git\n","!pip install transformers\n","!pip install git+https://github.com/ssut/py-hanspell.git\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd drive/MyDrive/MBTI\n","!pwd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DyiXeOjimgi9"},"outputs":[],"source":["import time\n","import datetime\n","import random\n","from tqdm import tqdm\n","\n","from dataloader import MBTIDataset\n","\n","import pandas as pd\n","pd.set_option('display.width', 180)\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n","from sklearn.model_selection import KFold\n","from transformers import DataCollatorWithPadding, BertForSequenceClassification, BertConfig, AdamW"]},{"cell_type":"markdown","metadata":{"id":"ARc_jwe5nF8g"},"source":["# Environment"]},{"cell_type":"code","execution_count":52,"metadata":{"id":"Ef5Z75ozmW8m","executionInfo":{"status":"ok","timestamp":1680360173620,"user_tz":-540,"elapsed":2,"user":{"displayName":"박상우","userId":"00596815453416531190"}}},"outputs":[],"source":["# Setup\n","env_dict = {\n","    # ==== Arguments for dataset =====\n","    'train_path'        : './data/hackathon_train_v1.csv',\n","    'question_path'     : './data/question_filtered.csv',\n","    'pretrained_url'    : \"klue/bert-base\",\n","    'padding_per_batch' : True,\n","    # ==== Arguments for dataloader =====\n","    'shuffle'           : False,            # turn off 'shuffle' since we use sampler in Dataloader\n","    # ==== Arguments for training =====\n","    'target'            : 'I/E',\n","    'lm'                : 'bert',\n","    'classifier'        : 'mlp',\n","    'batch_size'        : 64,\n","    'epoch'             : 5,\n","    'lr'                : 3e-3,\n","    'decay_rate'        : 1e-7,\n","    'dropout'           : 0.1,\n","    'hidden_dim'        : [192, 48, 12]     # 일단 설정해둔 숫자들 (cls token의 dimension인 768 을 4로 나눈 값들)\n","}\n","\n","# Random seed\n","seed_val = 1234\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)"]},{"cell_type":"code","source":["import torch, gc\n","gc.collect()\n","torch.cuda.empty_cache()"],"metadata":{"id":"EXNJJ7ZpWDwI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 지워질 Cell!!!!!\n","import os\n","import re\n","import time\n","import datetime\n","from tqdm import tqdm\n","from typing import Union\n","\n","import torch\n","import pandas as pd\n","from torch.utils.data import Dataset\n","from pykospacing import Spacing\n","from hanspell import spell_checker\n","from transformers import AutoTokenizer\n","\n","#TODO:\n","# 1. train / text 다른 로직이 필요 (데이터 형식이 조금 다름)\n","# 2. 다른 Text preprocess 방식 도입 검토\n","#       ref : https://ebbnflow.tistory.com/246\n","\n","class MBTIIDataset(Dataset):\n","    def __init__(\n","        self,\n","        data_path     : Union[str, pd.DataFrame],\n","        question_path : Union[str, pd.DataFrame],\n","        txt_preprocess: bool            = True,\n","        normalize     : bool            = True,\n","        pretrained_url: str             = \"klue/bert-base\",\n","        padding_per_batch               = True,\n","        is_binary_classification: bool  = True,\n","        is_bert       : bool            = True,\n","        is_train      : bool            = True\n","        ):\n","        \"\"\"DataLoader for MBTI dataset\n","\n","        Args:\n","            data_path (str): Data file path. Both csv and parguet files are allowed.\n","            question_path (str): Question file path. Both csv and parguet files are allowed.\n","            txt_preprocess (bool, optional): Text preprocessing pipeline. (e.g. fixing grammar, removing punctuations). Defaults to True.\n","            normalize (bool, optional): Normalize numeric attribute. Defaults to True.\n","            is_binary_classification (bool, optional): Target of task. You can choose btw Multi-class classificaiton\n","                and 4 binary classification problem. Defaults to True.\n","            is_bert (bool, optional): Using BERT for language model or not. Defaults to True.\n","            is_train (bool, optional): Whether given data is for training or not. Defaults to True.\n","        \"\"\"\n","        \n","        def resolve_path(path:str)->pd.DataFrame:\n","            if path.endswith('.csv'):\n","                try:\n","                    df = pd.read_csv(path)\n","                except:\n","                    df = pd.read_csv(path, encoding='cp949')\n","            else:\n","                df = pd.read_parquet(path)\n","            return df\n","\n","        data = None\n","        question_data = None\n","        label_cols = ['I/E', 'S/N', 'T/F', 'J/P']\n","        # if given data_path is pd.Dataframe, we assume preprocessing is already applied to given Dataframe\n","        # so that it can skip all the processes below\n","        if not isinstance(data_path, pd.DataFrame):\n","            data = resolve_path(data_path)\n","            question_data = resolve_path(question_path)\n","\n","            self.question_data = question_data\n","\n","            # preprocess data\n","            if txt_preprocess:\n","                self.preprocess_txt(data)\n","            if normalize:\n","                data['Age'] = (data['Age'] - data['Age'].mean()) / data['Age'].std()\n","\n","            # make dataset suitable for binary classification (only for training data - test data doesn't contain 'MBTI' field)\n","            if is_train and is_binary_classification:\n","                self.prepare_binary_classification(data)\n","                # if method right above works successfully, then data should contain same # 0 and 1.\n","                for col in label_cols:\n","                    value_counted = data[col].value_counts()\n","                    assert value_counted[0] == value_counted[1]\n","\n","            # prepare for language model\n","            self.tokenizer = AutoTokenizer.from_pretrained(pretrained_url)\n","            self.padding_per_batch = padding_per_batch\n","            self.tokenize(data)\n","\n","        else:\n","            data = data_path\n","\n","        # set columns for both training and inference\n","        #TODO: 테스트 데이터를 고려해서 유저 정보를 학습에 활용하지 않는 상황. 필요시 고쳐야 함\n","        self.cat_col    = ['Gender']\n","        self.num_col    = ['Age']\n","        self.label_cols = label_cols\n","        self.is_train   = is_train\n","        self.data       = data\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","\n","        selected_data = self.data.iloc[idx]\n","\n","        cat_input = torch.tensor(selected_data[self.cat_col])                               # [batch size   x   # categorical features]\n","        num_input = torch.tensor(selected_data[self.num_col])                               # [batch size   x   # numerical features]\n","\n","        sample              = selected_data['QandA']                                        # [batch size   x   sequence length]\n","        sample['cat_input'] = cat_input\n","        sample['num_input'] = num_input\n","\n","        # Include label only for training cases\n","        if self.is_train:\n","            for col in self.label_cols:\n","              label = torch.tensor(selected_data[col])                            # [batch size   x   1]\n","              sample[col] = label\n","\n","        return sample\n","\n","    # ======================\n","    #    Helper Functions\n","    # ======================\n","\n","    def fix_grammar(self, answer: str) -> str:\n","        answer = spell_checker.check(answer)\n","        return answer.checked\n","\n","    def fix_spacing(self, answer: str) -> str:\n","        answer  = answer.replace(\" \", '')\n","        spacing = Spacing()\n","        return spacing(answer)\n","\n","    def remove_punctuation(self, answer: str) -> str:\n","        answer = re.sub(r'[@%\\\\*=()/~#&\\+á?\\xc3\\xa1\\-\\|\\.\\:\\;\\!\\-\\,\\_\\~\\$\\'\\\"]', '', answer)\n","        answer = re.sub(r'\\s+', ' ', answer)        # remove extra space\n","        answer = re.sub(r\"^\\s+\", '', answer)        # remove space from start\n","        answer = re.sub(r'\\s+$', '', answer)        # remove space from the end\n","        return answer\n","\n","    def preprocess_txt(self, data: pd.DataFrame):\n","        try:\n","            data['Answer'] = data['Answer'].apply(self.fix_grammar)         #FIXME: 해당 패키지의 서버가 가끔 응답 오류가 남...\n","        except:\n","            pass\n","        tqdm.pandas()\n","        print('=============== fix_spacing ===============')\n","        data['Answer'] = data['Answer'].progress_apply(self.fix_spacing)\n","        tqdm.pandas()\n","        print('=============== remove_punctuation ===============')\n","        data['Answer'] = data['Answer'].progress_apply(self.remove_punctuation)\n","\n","    def prepare_binary_classification(self, data: pd.DataFrame):\n","        one_list = ['E', 'N', 'F', 'P']\n","        zero_list = ['I', 'S', 'T', 'J']\n","\n","        print('=============== prepare_binary_classification ===============')\n","        for idx, mbti in tqdm(enumerate(one_list)):\n","            data[mbti] = data['MBTI'].str               \\\n","                .contains(mbti)                         \\\n","                .replace({True: 1, False: 0})\n","\n","            new_name = zero_list[idx] + '/' + mbti\n","            data.rename(columns = {mbti:new_name}, inplace=True)\n","\n","    def tokenize(self, data: pd.DataFrame):\n","        \n","        #TODO: tokenizing tqdm 적용해두기\n","        def tokenize_per_sentence(series: pd.Series) -> str:\n","            selected_question = self.question_data.iloc[series['Q_number'] - 1].Question\n","            selected_answer = series['Answer']\n","\n","            padding = False if self.padding_per_batch else 'longest'\n","            #TODO: 필요시 max_length 조절 필요\n","            return self.tokenizer(selected_question,\n","                                  selected_answer,\n","                                  padding=padding)\n","\n","        tqdm.pandas()\n","        print('=============== tokenize ===============')\n","        data['QandA'] =  data.progress_apply(tokenize_per_sentence, axis=1)\n"],"metadata":{"id":"6NMyiI_B3LRs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Prepare dataset"],"metadata":{"id":"OmT20qIS3CfU"}},{"cell_type":"code","execution_count":64,"metadata":{"id":"MzufA7B9mW8o","colab":{"base_uri":"https://localhost:8080/","height":417,"referenced_widgets":["090f305c365f450590e7ec44f6420b86","2f1fc800454d496aa255fe21453e75bc","e8a6ea43298a49f4b993494be278a427","abb0412c2d1144f0aebadf3ebaa4141a","de31d42e027446dba6aea32d4ffaaffe","ce64c99491d143d492b8e0d24298b45e","03e0f5ef555141e6bee4de48760e52e0","00d7668a3ba74e85b16d01afa1e0392a","9a7d62b0fe7f41dc9134cf79e074a4ee","1a0061d62c094d76b9492e6ee18eb4ad","d84b56a181914136a33f0d2df4043ff9","d3c67afad9b24174bc182b50536de735","b91201adc0dd4fcab1dea378a60cb092","7272c26139244529b6cf3f7e98be962e","986509a827bd4625a77cc5472e38cc91","74df8728e2354fc49c2dcc25d7ef2521","2c5bd8a40208449d9da3b0b4541c8fbe","5632289d71ce468f9b1037a2dc823afb","58eba987d5c44a28b8f037f26f64aa65","1d2979abcc474cbb909870566474c2f2","00149b2f29904a9f95963ee94b84c2d3","6e87638533ea4c7f8ecf3276a572cf1f","2cf6219c8c8e418182c0f18f40fd41bb","7831675cbfff4362bbe3568c0ff96bcd","5f2f1930f6d44f7d8a31f351bf295923","16edba9260e94fe3a177975171c1ac29","819b263cd0c443eb9ebd00807994e711","f6feb19a21de4253aa105f279fd40bed","5dcd8b9506b847fb9eb58d0787f52137","4aa922c0222945938c3a4d482214d3c9","760204363f2f47b2ab870b2893b7326c","3a541f93c2a64f638361fe44edba5543","2d9ed683e10548809a08486f0920b963","eec26ccea7a341f6ab641b0dba57c9a6","33c79177b13842819b686739ff4d24aa","1c9eae54eb9b4ce9b781521291402bf7","bc841a8aa62a4bd5af0add98adb8a530","ed3ebd99c809401c9f04c45c906891df","704dcabda5144a459b825c7a4bdd7c47","9f3cf062429b4ed0a9d3252352a21c0c","eb366b03838041918c92f466b621cff3","62dff9cd682c4473bdaf659e5bcc3770","1ad6e084e1bb48528c030fde533e6066","cef70586735440aab9e4a9bac366ff68"]},"executionInfo":{"status":"ok","timestamp":1680360930861,"user_tz":-540,"elapsed":15644,"user":{"displayName":"박상우","userId":"00596815453416531190"}},"outputId":"128cda68-d6d7-4c04-fcb5-4439c91fb735"},"outputs":[{"output_type":"stream","name":"stdout","text":["=============== prepare_binary_classification ===============\n"]},{"output_type":"stream","name":"stderr","text":["4it [00:00, 65.74it/s]\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/289 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"090f305c365f450590e7ec44f6420b86"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/248k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3c67afad9b24174bc182b50536de735"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)/main/tokenizer.json:   0%|          | 0.00/495k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cf6219c8c8e418182c0f18f40fd41bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)cial_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eec26ccea7a341f6ab641b0dba57c9a6"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["=============== tokenize ===============\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 11520/11520 [00:05<00:00, 2034.80it/s]"]},{"output_type":"stream","name":"stdout","text":["11520\n","   Data_ID  User_ID  Gender       Age  MBTI  Q_number                                             Answer  I/E  S/N  T/F  J/P                                        QandA\n","0        1        1       1 -0.372581  INFP         1  <아니다> 어릴 때 왕따 당한 경험이 있고 외부 활동을 좋아하지 않기 때문에 소수의...    0    1    1    1  [input_ids, token_type_ids, attention_mask]\n","1        2        1       1 -0.372581  INFP         2  <중립>  다양한 관심사를 탐구하진 않지만 대체로 자연과 역사에 관련된 것을 좋아하...    0    1    1    1  [input_ids, token_type_ids, attention_mask]\n","2        3        1       1 -0.372581  INFP         3  <그렇다> 감정 이입이 잘되어 코미디 영화에서 사람이 울고 있을 때도 울기 때문에 ...    0    1    1    1  [input_ids, token_type_ids, attention_mask]\n","3        4        1       1 -0.372581  INFP         4  <중립> 대비책을 세우긴 하는데 세우다가 마는 편입니다. 일의 변수가 생길 수 있고...    0    1    1    1  [input_ids, token_type_ids, attention_mask]\n","4        5        1       1 -0.372581  INFP         5  <아니다> 평정심을 유지 못 하는 편입니다. 머릿속은 백지화가 된 상태로 말도 제대...    0    1    1    1  [input_ids, token_type_ids, attention_mask]\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["# Dataset\n","train_dataset = MBTIIDataset(\n","    data_path           = env_dict['train_path'],\n","    question_path       = env_dict['question_path'],\n","    pretrained_url      = env_dict['pretrained_url'],\n","    padding_per_batch   = env_dict['padding_per_batch'],\n","    txt_preprocess      = False,\n","    is_train            = True\n",")\n","\n","print(len(train_dataset))\n","print(train_dataset.data.head())"]},{"cell_type":"code","source":["# # # save preprocessed pd.Dataframe & TODO: tokenizer 도 같이 저장해줘야 함X -> Tokenizer 밖으로 빼주자\n","# data_path = './data/' + 'base_data.csv'\n","# # train_dataset.data.to_csv(data_path)"],"metadata":{"id":"r99ktXNBztto","executionInfo":{"status":"ok","timestamp":1680360498530,"user_tz":-540,"elapsed":1,"user":{"displayName":"박상우","userId":"00596815453416531190"}}},"execution_count":58,"outputs":[]},{"cell_type":"code","source":["# # Dataset from dataframe\n","# ttrain_dataset = MBTIIDataset(\n","#     data_path           = pd.read_csv(data_path),\n","#     question_path       = env_dict['question_path'],\n","#     pretrained_url      = env_dict['pretrained_url'],\n","#     padding_per_batch   = env_dict['padding_per_batch'],\n","#     is_train            = True\n","# )\n","\n","# print(len(ttrain_dataset))\n","# print(ttrain_dataset.data.info())"],"metadata":{"id":"kN0d5eTQWLNV"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":65,"metadata":{"id":"wBo4Xtc6oCls","executionInfo":{"status":"ok","timestamp":1680360970035,"user_tz":-540,"elapsed":1,"user":{"displayName":"박상우","userId":"00596815453416531190"}}},"outputs":[],"source":["# define collator function when padding per batch is needed\n","#TODO: data_collator가 아닌 torch의 Packing 을 이용하는 것과 성능 비교가 필요함\n","data_collator = DataCollatorWithPadding(tokenizer=train_dataset.tokenizer) if env_dict['padding_per_batch'] else None\n","\n","# Dataloader\n","train_dataloader = DataLoader(\n","    train_dataset,\n","    batch_size  = env_dict['batch_size'],\n","    shuffle     = env_dict['shuffle'],\n","    collate_fn  = data_collator\n",")"]},{"cell_type":"markdown","metadata":{"id":"60pvecWkpznO"},"source":["# Prepare language model"]},{"cell_type":"code","execution_count":67,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1067,"status":"ok","timestamp":1680360994428,"user":{"displayName":"박상우","userId":"00596815453416531190"},"user_tz":-540},"id":"tbVO_n8BQ1bc","outputId":"0ad3db11-e03a-4e3f-a3ff-aa1686b42057"},"outputs":[{"output_type":"stream","name":"stdout","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla T4\n"]}],"source":["# GPU preparation\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","else:\n","    device = torch.device(\"cpu\")\n","    print('No GPU available, using CPU instead.')"]},{"cell_type":"code","source":["from mlp import MLPClassifier\n","\n","class BertWithMlp(BertForSequenceClassification):\n","    def __init__(\n","        self,\n","        config,\n","        input_dim = None,\n","        hidden_dim = None,\n","        num_classes = 2,\n","        dropout = 0.1\n","        ):\n","\n","        # ====================\n","        #      BERT Setup\n","        # ====================\n","\n","        # resulting BERT model is stored in 'self.bert'.\n","        super().__init__(config)\n","\n","        self.num_labels = config.num_labels\n","\n","        # 나중에 config 내부에 해당 field 값을 넣어주면 됨 (영상 참고)\n","        combined_feat_dim = config.text_feat_dim + config.cat_feat_dim + config.num_feat_dim\n","        print(combined_feat_dim)\n","\n","        self.mlp = MLPClassifier(\n","            combined_feat_dim,\n","            None,\n","            hidden_dim,\n","            num_classes=num_classes,\n","            dropout=dropout\n","        )\n","        self.dropout = nn.Dropout(p=dropout, inplace=False)\n","        self.bn = nn.BatchNorm1d(config.num_feat_dim)\n","\n","    def forward(\n","        self,\n","        input_ids = None,\n","        attention_mask = None,\n","        token_type_ids = None,\n","        position_ids = None,\n","        head_mask = None,\n","        inputs_embeds = None,\n","        labels = None,\n","        output_attentions = None,\n","        cat_feats = None,\n","        num_feats = None\n","    ):\n","        # ====================\n","        #     BERT forward\n","        # ====================\n","        #TODO: 더 많은 인자 추가해주기\n","        logits = self.bert(\n","            input_ids,\n","            token_type_ids=token_type_ids,\n","            attention_mask=attention_mask)\n","\n","        cls = logits[1]\n","        # Apply dropout to cls\n","        cls = self.dropout(cls)\n","        # Apply batch normalization to numerical features\n","        # num_feats = self.bn(num_feats)\n","\n","        print(\"cls shape :\", cls.shape)\n","        print(\"cat shape :\", cat_feats.shape)\n","        print(\"num shape :\", num_feats.shape)\n","\n","        # ====================\n","        #      MLP forward\n","        # ====================\n","        all_feats = torch.cat((cls, cat_feats.view(-1, 1), num_feats.view(-1, 1)), dim=1)\n","        output = self.mlp(all_feats)\n","\n","        return output\n"],"metadata":{"id":"ZpVnMOF-9yDs","executionInfo":{"status":"ok","timestamp":1680361601619,"user_tz":-540,"elapsed":1,"user":{"displayName":"박상우","userId":"00596815453416531190"}}},"execution_count":90,"outputs":[]},{"cell_type":"code","source":["# Update config file\n","from transformers import BertConfig\n","\n","#TODO: num_labels 인자가 필요한지 알아봐야 함\n","config = BertConfig.from_pretrained(\n","    env_dict['pretrained_url'],\n","    num_labels = 2\n","  )\n","\n","config.num_feat_dim = 1\n","config.cat_feat_dim = 1\n","config.text_feat_dim = config.hidden_size\n","\n","print(config)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wHzf_UILKiZW","executionInfo":{"status":"ok","timestamp":1680361467322,"user_tz":-540,"elapsed":627,"user":{"displayName":"박상우","userId":"00596815453416531190"}},"outputId":"35218232-d14b-45b7-af80-7c255300c56a"},"execution_count":85,"outputs":[{"output_type":"stream","name":"stdout","text":["BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"cat_feat_dim\": 1,\n","  \"classifier_dropout\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_feat_dim\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"text_feat_dim\": 768,\n","  \"transformers_version\": \"4.27.4\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 32000\n","}\n","\n"]}]},{"cell_type":"code","source":["# Prepare model\n","model = BertWithMlp.from_pretrained(\n","    env_dict['pretrained_url'],\n","    config = config,\n","    hidden_dim = env_dict['hidden_dim'],\n","    dropout = env_dict['dropout']\n","    )\n","model.cuda()\n","\n","# Apply weight decaying except for bias & layer normalization term\n","no_decay = ['bias', 'LayerNorm.weight']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","]\n","\n","# Prepare optimizer\n","optimizer = AdamW(optimizer_grouped_parameters, lr=env_dict['lr'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QwILgWVI-ma5","executionInfo":{"status":"ok","timestamp":1680361606644,"user_tz":-540,"elapsed":3385,"user":{"displayName":"박상우","userId":"00596815453416531190"}},"outputId":"0e7e20dd-2855-4849-a962-e76e51b10855"},"execution_count":91,"outputs":[{"output_type":"stream","name":"stdout","text":["770\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at klue/bert-base were not used when initializing BertWithMlp: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertWithMlp from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertWithMlp from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertWithMlp were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.weight', 'mlp.layers.3.bias', 'bn.num_batches_tracked', 'bn.running_mean', 'mlp.layers.2.bias', 'mlp.layers.3.weight', 'mlp.layers.1.bias', 'mlp.layers.0.bias', 'bn.weight', 'bn.bias', 'mlp.layers.0.weight', 'bn.running_var', 'mlp.layers.1.weight', 'classifier.bias', 'mlp.layers.2.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["# Freeze Encoder, use head's parameters only\n","free_encoder = True\n","\n","if free_encoder:\n","  for param in model.base_model.parameters():\n","    param.requires_grad = False"],"metadata":{"id":"aSqmFz0aiSph","executionInfo":{"status":"ok","timestamp":1680361607841,"user_tz":-540,"elapsed":4,"user":{"displayName":"박상우","userId":"00596815453416531190"}}},"execution_count":92,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xsg8RoRjXjQE"},"source":["# Train"]},{"cell_type":"code","source":["def format_time(elapsed):\n","    elapsed_rounded = int(round((elapsed)))\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"],"metadata":{"id":"5LN0_mWU8Yvj","executionInfo":{"status":"ok","timestamp":1680359098353,"user_tz":-540,"elapsed":2,"user":{"displayName":"박상우","userId":"00596815453416531190"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","source":["# Set the target of training right before training loop\n","env_dict['target_of_training'] = 'I/E'"],"metadata":{"id":"9Z6v5b5ukwBb","executionInfo":{"status":"ok","timestamp":1680361023133,"user_tz":-540,"elapsed":1,"user":{"displayName":"박상우","userId":"00596815453416531190"}}},"execution_count":69,"outputs":[]},{"cell_type":"code","source":["# Training Loop for 영도\n","model.train()\n","\n","for epoch in range(env_dict['epoch']):\n","  t0 = time.time()\n","  epoch_loss = 0\n","  for step, batch in tqdm(enumerate(train_dataloader)):\n","\n","    input_ids       = batch['input_ids'].to(device)\n","    token_type_ids  = batch['token_type_ids'].to(device)\n","    attention_mask  = batch['attention_mask'].to(device)\n","    cat_input       = batch['cat_input'].to(device)\n","    num_input       = batch['num_input'].to(device)\n","    labels  = batch[env_dict['target_of_training']].to(device)\n","\n","    # Clear prior gradients\n","    model.zero_grad()\n","\n","    # Forward\n","    output = model(input_ids,\n","                    token_type_ids=token_type_ids,\n","                    attention_mask=attention_mask,\n","                    cat_feats=cat_input,\n","                    num_feats=num_input)\n","\n","    # Calculate loss - CELoss 함수 짜서 실험 좀 부탁해유~\n","\n","    print(output)\n","\n","    break\n","\n","\n","    print(\"Step loss: {0:.2f}\".format(loss))\n","\n","    loss.backward()\n","    optimizer.step()\n","\n","  avg_train_loss = total_loss / len(train_dataloader)\n","  print(\"Epoch loss: {0:.2f}\".format(loss))\n","  print(\"Training epoch took: {:}\".format(format_time(time.time() - t0)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":481},"id":"_2U999FINgOv","executionInfo":{"status":"error","timestamp":1680361608774,"user_tz":-540,"elapsed":935,"user":{"displayName":"박상우","userId":"00596815453416531190"}},"outputId":"21d9f1dc-177b-4d17-a102-fc0be5c29808"},"execution_count":93,"outputs":[{"output_type":"stream","name":"stderr","text":["0it [00:00, ?it/s]\n"]},{"output_type":"stream","name":"stdout","text":["torch.Size([64, 768])\n","torch.Size([64])\n","torch.Size([64])\n","torch.Size([64, 770])\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-93-888d5b68782c>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     output = model(input_ids,\n\u001b[0m\u001b[1;32m     21\u001b[0m                     \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-90-50f77c5bf75f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, cat_feats, num_feats)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mall_feats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat_feats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_feats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_feats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_feats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/My Drive/MBTI/mlp.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;31m# If layer is not the last layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype"]}]},{"cell_type":"markdown","metadata":{"id":"1LUMI4BiZVac"},"source":["# Save trained model"]},{"cell_type":"code","source":["# Save\n","#TODO: haperparams가 이름에 드러날 수 있는 저장경로 생각해보기\n","save_path = './models/' + env_dict['lm'] + 'with' + env_dict['classifier'] + '.pt'\n","torch.save(model.state_dict(), save_path)"],"metadata":{"id":"_rnIRUdoNfSo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 저장된 모델을 다른 파일에 불러와서 Test 하길 권장! (test.py 만들어도 좋아)"],"metadata":{"id":"RGy7ri7aRjed"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# K-Fold Cross Validation (수정 중)"],"metadata":{"id":"4iVUGgx9NXeT"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"R-DVssQfmW8p"},"outputs":[],"source":["# Training with cross validation (ref : https://velog.io/@pppanghyun/6.-%EA%B5%90%EC%B0%A8-%EA%B2%80%EC%A6%9DCross-Validation)\n","#TODO: Scheduler, Gradient Clipping\n","\n","kfold     = KFold(n_splits=5, shuffle=True)\n","criterion = torch.nn.MSELoss()\n","\n","validation_loss = []\n","\n","for fold, (train_idx, val_idx) in enumerate(kfold.split(train_dataset)):\n","\n","    #TODO: Fold 끼리 겹칠 수 있음. 겹치지 않는 방식 고려 필요\n","    # Make indices for both training and validation\n","    train_subsampler  = SubsetRandomSampler(train_idx)\n","    val_subsampler    = SubsetRandomSampler(val_idx)\n","\n","    # Define dataloader using sampler\n","    train_dataloder = DataLoader(\n","        train_dataset,\n","        batch_size  = env_dict['batch_size'],\n","        shuffle     = env_dict['shuffle'],\n","        sampler     = train_subsampler,\n","        collate_fn  = data_collator\n","    )\n","    val_dataloder   = DataLoader(\n","        train_dataset,\n","        batch_size  = env_dict['batch_size'],\n","        shuffle     = env_dict['shuffle'],\n","        sampler     = val_subsampler,\n","        collate_fn  = data_collator\n","    )\n","\n","    # ===================\n","    #    Training Loop\n","    # ===================\n","\n","    optimizer = AdamW(model.parameters(), lr=env_dict['lr'], weight_decay=env_dict['decay_rate'])\n","\n","    model.train()\n","\n","    for epoch in range(env_dict['epoch']):\n","      t0 = time.time()\n","      epoch_loss = 0\n","      for step, batch in tqdm(enumerate(train_dataloader)):\n","\n","        input_ids       = batch['input_ids'].to(device)\n","        token_type_ids  = batch['token_type_ids'].to(device)\n","        attention_mask  = batch['attention_mask'].to(device)\n","        labels  = batch[env_dict['target_of_training']].to(device)\n","\n","        # Clear prior gradients\n","        model.zero_grad()\n","\n","        # Forward\n","        output = model(input_ids,\n","                       token_type_ids=token_type_ids,\n","                       attention_mask=attention_mask,\n","                       cat_feats=batch['cat_input'],\n","                       num_feats=batch['num_input'])\n","\n","        # Calculate loss\n","\n","        # loss    = outputs.loss     # Default : CELoss\n","\n","        print(\"Step loss: {0:.2f}\".format(loss))\n","\n","        loss.backward()\n","        optimizer.step()\n","      \n","      avg_train_loss = total_loss / len(train_dataloader)\n","      print(\"Epoch loss: {0:.2f}\".format(loss))\n","      print(\"Training epoch took: {:}\".format(format_time(time.time() - t0)))\n","\n","#     train_rmse = evaluation(trainloader) # 학습 데이터의 RMSE\n","#     val_rmse = evaluation(valloader)\n","#     print(\"k-fold\", fold,\" Train Loss: %.4f, Validation Loss: %.4f\" %(train_rmse, val_rmse)) \n","#     validation_loss.append(val_rmse)\n","\n","## Calculate validation score\n","\n","# validation_loss = np.array(validation_loss)\n","# mean = np.mean(validation_loss)\n","# std = np.std(validation_loss)\n","# print(\"Validation Score: %.4f, ± %.4f\" %(mean, std))"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"toc_visible":true},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.9.7 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"7337906654535f92ad925780206611836a0922d3540b9d89ec64204ed4b58f9b"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"090f305c365f450590e7ec44f6420b86":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2f1fc800454d496aa255fe21453e75bc","IPY_MODEL_e8a6ea43298a49f4b993494be278a427","IPY_MODEL_abb0412c2d1144f0aebadf3ebaa4141a"],"layout":"IPY_MODEL_de31d42e027446dba6aea32d4ffaaffe"}},"2f1fc800454d496aa255fe21453e75bc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ce64c99491d143d492b8e0d24298b45e","placeholder":"​","style":"IPY_MODEL_03e0f5ef555141e6bee4de48760e52e0","value":"Downloading (…)okenizer_config.json: 100%"}},"e8a6ea43298a49f4b993494be278a427":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_00d7668a3ba74e85b16d01afa1e0392a","max":289,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9a7d62b0fe7f41dc9134cf79e074a4ee","value":289}},"abb0412c2d1144f0aebadf3ebaa4141a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1a0061d62c094d76b9492e6ee18eb4ad","placeholder":"​","style":"IPY_MODEL_d84b56a181914136a33f0d2df4043ff9","value":" 289/289 [00:00&lt;00:00, 6.67kB/s]"}},"de31d42e027446dba6aea32d4ffaaffe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ce64c99491d143d492b8e0d24298b45e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"03e0f5ef555141e6bee4de48760e52e0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"00d7668a3ba74e85b16d01afa1e0392a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9a7d62b0fe7f41dc9134cf79e074a4ee":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1a0061d62c094d76b9492e6ee18eb4ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d84b56a181914136a33f0d2df4043ff9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d3c67afad9b24174bc182b50536de735":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b91201adc0dd4fcab1dea378a60cb092","IPY_MODEL_7272c26139244529b6cf3f7e98be962e","IPY_MODEL_986509a827bd4625a77cc5472e38cc91"],"layout":"IPY_MODEL_74df8728e2354fc49c2dcc25d7ef2521"}},"b91201adc0dd4fcab1dea378a60cb092":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2c5bd8a40208449d9da3b0b4541c8fbe","placeholder":"​","style":"IPY_MODEL_5632289d71ce468f9b1037a2dc823afb","value":"Downloading (…)solve/main/vocab.txt: 100%"}},"7272c26139244529b6cf3f7e98be962e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_58eba987d5c44a28b8f037f26f64aa65","max":248477,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1d2979abcc474cbb909870566474c2f2","value":248477}},"986509a827bd4625a77cc5472e38cc91":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_00149b2f29904a9f95963ee94b84c2d3","placeholder":"​","style":"IPY_MODEL_6e87638533ea4c7f8ecf3276a572cf1f","value":" 248k/248k [00:00&lt;00:00, 374kB/s]"}},"74df8728e2354fc49c2dcc25d7ef2521":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2c5bd8a40208449d9da3b0b4541c8fbe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5632289d71ce468f9b1037a2dc823afb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"58eba987d5c44a28b8f037f26f64aa65":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1d2979abcc474cbb909870566474c2f2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"00149b2f29904a9f95963ee94b84c2d3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e87638533ea4c7f8ecf3276a572cf1f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2cf6219c8c8e418182c0f18f40fd41bb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7831675cbfff4362bbe3568c0ff96bcd","IPY_MODEL_5f2f1930f6d44f7d8a31f351bf295923","IPY_MODEL_16edba9260e94fe3a177975171c1ac29"],"layout":"IPY_MODEL_819b263cd0c443eb9ebd00807994e711"}},"7831675cbfff4362bbe3568c0ff96bcd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f6feb19a21de4253aa105f279fd40bed","placeholder":"​","style":"IPY_MODEL_5dcd8b9506b847fb9eb58d0787f52137","value":"Downloading (…)/main/tokenizer.json: 100%"}},"5f2f1930f6d44f7d8a31f351bf295923":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4aa922c0222945938c3a4d482214d3c9","max":494860,"min":0,"orientation":"horizontal","style":"IPY_MODEL_760204363f2f47b2ab870b2893b7326c","value":494860}},"16edba9260e94fe3a177975171c1ac29":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3a541f93c2a64f638361fe44edba5543","placeholder":"​","style":"IPY_MODEL_2d9ed683e10548809a08486f0920b963","value":" 495k/495k [00:00&lt;00:00, 559kB/s]"}},"819b263cd0c443eb9ebd00807994e711":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f6feb19a21de4253aa105f279fd40bed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5dcd8b9506b847fb9eb58d0787f52137":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4aa922c0222945938c3a4d482214d3c9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"760204363f2f47b2ab870b2893b7326c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3a541f93c2a64f638361fe44edba5543":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d9ed683e10548809a08486f0920b963":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eec26ccea7a341f6ab641b0dba57c9a6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_33c79177b13842819b686739ff4d24aa","IPY_MODEL_1c9eae54eb9b4ce9b781521291402bf7","IPY_MODEL_bc841a8aa62a4bd5af0add98adb8a530"],"layout":"IPY_MODEL_ed3ebd99c809401c9f04c45c906891df"}},"33c79177b13842819b686739ff4d24aa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_704dcabda5144a459b825c7a4bdd7c47","placeholder":"​","style":"IPY_MODEL_9f3cf062429b4ed0a9d3252352a21c0c","value":"Downloading (…)cial_tokens_map.json: 100%"}},"1c9eae54eb9b4ce9b781521291402bf7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_eb366b03838041918c92f466b621cff3","max":125,"min":0,"orientation":"horizontal","style":"IPY_MODEL_62dff9cd682c4473bdaf659e5bcc3770","value":125}},"bc841a8aa62a4bd5af0add98adb8a530":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1ad6e084e1bb48528c030fde533e6066","placeholder":"​","style":"IPY_MODEL_cef70586735440aab9e4a9bac366ff68","value":" 125/125 [00:00&lt;00:00, 6.48kB/s]"}},"ed3ebd99c809401c9f04c45c906891df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"704dcabda5144a459b825c7a4bdd7c47":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f3cf062429b4ed0a9d3252352a21c0c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eb366b03838041918c92f466b621cff3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"62dff9cd682c4473bdaf659e5bcc3770":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1ad6e084e1bb48528c030fde533e6066":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cef70586735440aab9e4a9bac366ff68":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}